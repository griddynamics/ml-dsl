{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Net Diagnostics Ingest Management</center></h1>\n",
    "<a id=\"tc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. [Configuration](#configuration) \n",
    "2. [Start ND Ingest Part 0 (TEST)](#ingest0)\n",
    "3. [Start ND Ingest Part 1](#ingest1)\n",
    "4. [Start ND Ingest Part 2](#ingest2)\n",
    "5. [Start ND Ingest Transaction Part 1](#ingest3)\n",
    "6. [Start ND Ingest Transaction Part 2](#ingest4)\n",
    "7. [Start ND Augmentation](#augmentation)\n",
    "8. [Remove Duplicates](#rmduplicates)\n",
    "9. [Push Metrics to DB](#todb)\n",
    "10. [Prepare ND LGBM Signature](#signature)\n",
    "11. [Train and Tune LGBM Model](#hptuning)\n",
    "12. [Trained Model Deployment](#deployment)\n",
    "13. [Cold Start Prediction before Anomaly Detection](#coldstart)\n",
    "14. [Batch Prediction](#prediction)\n",
    "15. [Batch Anomaly Detection for Test](#detection)\n",
    "16. [Analytics](#analytics)\n",
    "17. [Streaming](#streaming)\n",
    "18. [Push Notebook to GCS Bucket](#gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "## Configuration\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_transition(transition_file):\n",
    "    with open(transition_file, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH='/home/jovyan/work/data'\n",
    "PROJECT_PATH = f'{BASE_PATH}/poc'\n",
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "PROJECT = 'kohls-kos-cicd'\n",
    "CLUSTER = 'ai4ops'\n",
    "REGION='global'\n",
    "AI_PLATFORM_REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldsl import *\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyspark\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest0\"></a>\n",
    "## Start ND Ingest Part 0 (TEST)\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = f\"{PROJECT_PATH}/spark/ingest\"\n",
    "RESOURCES='/opt/dataproc/.resources'\n",
    "\n",
    "DURATION = '1' # seconds\n",
    "POOL_SIZE = '2'\n",
    "TIMEOUT = '1440' # minutes\n",
    "WRITE_FORMAT = 'csv'\n",
    "SLICE_SIZE = '10000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = GCPSessionFactory.build_session(job_bucket=BUCKET,job_region=REGION, cluster=CLUSTER, job_project_id=PROJECT, \n",
    "                                          ml_region=AI_PLATFORM_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME='job_part_kohls_nd_08_00.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {'--token_file_gcs_path':f'gs://{BUCKET}/resources/kohls_nd.txt',\\\n",
    "             '--res_path':RESOURCES,\\\n",
    "             '--duration':DURATION,\\\n",
    "             '--pool_size':POOL_SIZE,\\\n",
    "             '--timeout':TIMEOUT,\\\n",
    "             '--write_format':WRITE_FORMAT,\\\n",
    "             '--slice_size':SLICE_SIZE\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = int(datetime.now().timestamp())\n",
    "\n",
    "test_nd_job_name = f\"api_ai4ops_ingest_from_nd_{TIMESTAMP}\"\n",
    "\n",
    "arguments['--output_file_pattern_path'] = f'gs://{BUCKET}/nd_history/{test_nd_job_name}'\n",
    "arguments['--tasks_file_path'] = CONFIG_NAME\n",
    "\n",
    "test_nd_job = builder.job_file(f'{SCRIPT_PATH}/nd_history_ingest_batch.py')\\\n",
    ".job_id(test_nd_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "test_nd_executor = DataprocExecutor(test_nd_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_ai4ops_ingest_from_nd_1567572118 was submitted to the cluster ai4ops\n",
      "Job STATUS was set to PENDING at 2019-09-04 04:41:15\n",
      "Job STATUS was set to SETUP_DONE at 2019-09-04 04:41:17\n",
      "      Yarn APP /home/jovyan/work/data/poc/spark/ingest/nd_history_ingest_batch.py with STATUS ACCEPTED has PROGRESS 0\n",
      "      Yarn APP /home/jovyan/work/data/poc/spark/ingest/nd_history_ingest_batch.py with STATUS RUNNING has PROGRESS 10\n",
      "Canceling job: api_ai4ops_ingest_from_nd_1567572118\n"
     ]
    }
   ],
   "source": [
    "test_nd_executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest2\"></a>\n",
    "## Start ND Ingest Part 1\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_NAME='job_part_kohls_nd_08_ALL_01.json'\n",
    "CONFIG_NAME='job_part_kohls_nd_PEAK_ALL_01.json'\n",
    "\n",
    "DURATION = '5' # seconds\n",
    "POOL_SIZE = '1'\n",
    "TIMEOUT = '1440' # minutes\n",
    "WRITE_FORMAT = 'csv'\n",
    "SLICE_SIZE = '20000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {'--token_file_gcs_path':f'gs://{BUCKET}/resources/kohls_nd.txt',\\\n",
    "             '--res_path':RESOURCES,\\\n",
    "             '--duration':DURATION,\\\n",
    "             '--pool_size':POOL_SIZE,\\\n",
    "             '--timeout':TIMEOUT,\\\n",
    "             '--write_format':WRITE_FORMAT,\\\n",
    "             '--slice_size':SLICE_SIZE\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = int(datetime.now().timestamp())\n",
    "\n",
    "part1_nd_job_name = f\"api_ai4ops_ingest_from_nd_{TIMESTAMP}\"\n",
    "\n",
    "arguments['--output_file_pattern_path'] = f'gs://{BUCKET}/nd_history/{part1_nd_job_name}'\n",
    "arguments['--tasks_file_path'] = CONFIG_NAME'\n",
    "\n",
    "part1_nd_job = builder.job_file(f'{SCRIPT_PATH}/nd_history_ingest_batch.py')\\\n",
    ".job_id(part1_nd_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "part1_nd_executor = DataprocExecutor(part1_nd_job, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest2\"></a>\n",
    "## Start ND Ingest Part 2\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_NAME='job_part_kohls_nd_08_ALL_02.json'\n",
    "CONFIG_NAME='job_part_kohls_nd_PEAK_ALL_02.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = int(datetime.now().timestamp())\n",
    "\n",
    "part2_nd_job_name = f\"api_ai4ops_ingest_from_nd_{TIMESTAMP}\"\n",
    "\n",
    "arguments['--output_file_pattern_path'] = f'gs://{BUCKET}/nd_history/{part2_nd_job_name}'\n",
    "arguments['--tasks_file_path'] = CONFIG_NAME\n",
    "\n",
    "part2_nd_job = builder.job_file(f'{SCRIPT_PATH}/nd_history_ingest_batch.py')\\\n",
    ".job_id(part2_nd_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "part2_nd_executor = DataprocExecutor(part2_nd_job, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest3\"></a>\n",
    "## Start ND Ingest Transactions Part 1\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_NAME='job_part_kohls_nd_08_ALL_TX_01.json'\n",
    "CONFIG_NAME='job_part_kohls_nd_PEAK_ALL_TX_01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = int(datetime.now().timestamp())\n",
    "\n",
    "part1_tx_nd_job_name = f\"api_ai4ops_ingest_from_nd_{TIMESTAMP}\"\n",
    "\n",
    "arguments['--output_file_pattern_path'] = f'gs://{BUCKET}/nd_history/{part1_tx_nd_job_name}'\n",
    "arguments['--tasks_file_path'] = CONFIG_NAME\n",
    "\n",
    "part1_tx_nd_job = builder.job_file(f'{SCRIPT_PATH}/nd_history_ingest_batch.py')\\\n",
    ".job_id(part1_tx_nd_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "part1_tx_nd_executor = DataprocExecutor(part1_tx_nd_job, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest3\"></a>\n",
    "## Start ND Ingest Transactions Part 2\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_NAME='job_part_kohls_nd_08_ALL_TX_02.json'\n",
    "CONFIG_NAME='job_part_kohls_nd_PEAK_ALL_TX_02.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = int(datetime.now().timestamp())\n",
    "\n",
    "part2_tx_nd_job_name = f\"api_ai4ops_ingest_from_nd_{TIMESTAMP}\"\n",
    "\n",
    "arguments['--output_file_pattern_path'] = f'gs://{BUCKET}/nd_history/{part2_tx_nd_job_name}'\n",
    "arguments['--tasks_file_path'] = CONFIG_NAME\n",
    "\n",
    "part2_tx_nd_job = builder.job_file(f'{SCRIPT_PATH}/nd_history_ingest_batch.py')\\\n",
    ".job_id(part2_tx_nd_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "part2_tx_nd_executor = DataprocExecutor(part2_tx_nd_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_nd_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_nd_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_tx_nd_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_tx_nd_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state1 = part1_nd_executor.get_job_state()\n",
    "state2 = part2_nd_executor.get_job_state()\n",
    "state3 = part1_tx_nd_executor.get_job_state()\n",
    "state4 = part2_tx_nd_executor.get_job_state()\n",
    "\n",
    "print('State 1: {}'.format(state1))\n",
    "print('State 2: {}'.format(state2))\n",
    "print('State 3: {}'.format(state3))\n",
    "print('State 4: {}'.format(state4))\n",
    "\n",
    "if state1 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "if state2 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "if state3 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "if state4 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_ingest_transition = {\n",
    "    \"INGEST_JOB_1\": f\"{part1_nd_job_name.job_id}\",\n",
    "    \"INGEST_JOB_2\": f\"{part2_nd_job_name.job_id}\",\n",
    "    \"INGEST_TX_JOB_1\": f\"{part1_tx_nd_job_name.job_id}\",\n",
    "    \"INGEST_TX_JOB_2\": f\"{part2_tx_nd_job_name.job_id}\",\n",
    "    \"INGEST_TIMESTAMP\": f\"{int(datetime.now().timestamp())}\",\n",
    "    \"INGEST_BUCKET\": f\"{BUCKET}\",\n",
    "    \"INGEST_OUTPUT_JOB_1\": f\"gs://{BUCKET}/nd_history/{part1_nd_job_name}/chunk*\",\n",
    "    \"INGEST_OUTPUT_JOB_2\": f\"gs://{BUCKET}/nd_history/{part2_nd_job_name}/chunk*\",\n",
    "    \"INGEST_OUTPUT_TX_JOB_1\": f\"gs://{BUCKET}/nd_history/{part1_tx_nd_job_name}/chunk*\",\n",
    "    \"INGEST_OUTPUT_TX_JOB_2\": f\"gs://{BUCKET}/nd_history/{part2_tx_nd_job_name}/chunk*\",\n",
    "    \"INGEST_STATE_JOB_1\": f\"{state1}\",\n",
    "    \"INGEST_STATE_JOB_2\": f\"{state2}\",\n",
    "    \"INGEST_STATE_TX_JOB_1\": f\"{state3}\",\n",
    "    \"INGEST_STATE_TX_JOB_2\": f\"{state4}\"\n",
    "}\n",
    "\n",
    "with open('api_nd_transition_ingest.json', 'w') as file:\n",
    "     file.write(json.dumps(nd_ingest_transition)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"augmentation\"></a>\n",
    "## Start ND Augmentation\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# August\n",
    "# SOURCE_PART_1='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567190234/chunk*'\n",
    "# SOURCE_PART_2='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567190231/chunk*'\n",
    "# SOURCE_PART_3='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567190227/chunk*'\n",
    "# SOURCE_PART_4='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567190222/chunk*'\n",
    "\n",
    "# # Peak\n",
    "# SOURCE_PART_1='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567289986/chunk*'\n",
    "# SOURCE_PART_2='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567289983/chunk*'\n",
    "# SOURCE_PART_3='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567289969/chunk*'\n",
    "# SOURCE_PART_4='gs://ai4ops-main-storage-bucket/nd_history/ai4ops_ingest_from_net_diagnostics_1567289961/chunk*'\n",
    "\n",
    "\n",
    "nd_ingest_transition = get_transition('api_nd_transition_ingest.json')\n",
    "print(nd_ingest_transition)\n",
    "INGEST_OUTPUT_JOB_1 = transition.get('INGEST_OUTPUT_JOB_1', '')\n",
    "INGEST_OUTPUT_JOB_2 = transition.get('INGEST_OUTPUT_JOB_2', '')\n",
    "INGEST_OUTPUT_TX_JOB_1 = transition.get('INGEST_OUTPUT_TX_JOB_1', '')\n",
    "INGEST_OUTPUT_TX_JOB_2 = transition.get('INGEST_OUTPUT_TX_JOB_2', '')\n",
    "INPUT_PATH = f'{INGEST_OUTPUT_JOB_1},{INGEST_OUTPUT_JOB_2},{INGEST_OUTPUT_TX_JOB_1},{INGEST_OUTPUT_TX_JOB_2}'\n",
    "\n",
    "\n",
    "\n",
    "DATA_START_DATE = '2018-11-09T04:21:00Z'\n",
    "DATA_END_DATE = '2018-12-10T04:21:00Z'\n",
    "\n",
    "DATA_BASE_PATH = 'nd_history'\n",
    "\n",
    "ND_HIST_BASE_PATH = f'gs://{BUCKET}/{DATA_BASE_PATH}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "nd_aug_job_name = \"api_ai4ops_nd_augmentation_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "AUGMENTATION_OUT=f'{ND_HIST_BASE_PATH}/augmented/{nd_aug_job_name}'\n",
    "\n",
    "arguments = {\"--input_data_path\":INPUT_PATH,\\\n",
    "        \"--output_data_path\":AUGMENTATION_OUT, \\\n",
    "        \"--start_date\":DATA_START_DATE,\"--end_date\":DATA_END_DATE}\n",
    "\n",
    "augumentation_job = builder.task_script(f'{SCRIPT_PATH}/nd_augmentation.py')\\\n",
    ".job_id(nd_aug_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "nd_aug_executor = DataprocExecutor(augumentation_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_res = nd_aug_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = nd_aug_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_nd_augmentation = {\n",
    "    \"AUGMENTATION_JOB\": nd_aug_job_name,\n",
    "    \"AUGMENTATION_OUTPUT\": AUGMENTATION_OUT,\n",
    "    \"AUGMENTATION_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_nd_augmentation.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_nd_augmentation)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rmduplicates\"></a>\n",
    "## Remove Duplicates\n",
    "[back to Table Of Contents](#tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "\n",
    "transition = get_transition('api_transition_nd_augmentation.json')\n",
    "print(transition)\n",
    "AUGMENTATION_OUT = transition.get('AUGMENTATION_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_job_name = \"api_remove_duplicates_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "DEDUPLICATION_OUT=f'{BASE_PATH}/no_duplicates/{dedup_job_name}'\n",
    "\n",
    "arguments = {\"--input_data_path\":f\"{AUGMENTATION_OUT}/chunk*\",\\\n",
    "        \"--output_data_path\":DEDUPLICATION_OUT}\n",
    "\n",
    "deduplication_job = builder.task_script('remove_duplicates.py')\\\n",
    ".job_id(dedup_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "dedup_executor = DataprocExecutor(deduplication_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_res = dedup_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = dedup_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplication_transition = {\n",
    "    \"REMOVE_DUPLICATES_JOB\": dedup_job_name,\n",
    "    \"REMOVE_DUPLICATES_OUTPUT\": DEDUPLICATION_OUT,\n",
    "    \"REMOVE_DUPLICATES_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_remove_duplicates.json', 'w') as file:\n",
    "     file.write(json.dumps(deduplication_transition)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todb\"></a>\n",
    "## Push Metrics to DB\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation ND Metrics to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = get_transition('api_transition_remove_duplicates.json')\n",
    "print(transition)\n",
    "DEDUPLICATION_OUT = transition.get('REMOVE_DUPLICATES_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SECRET=\"kohls_db.txt\"\n",
    "METRIC_FILTER = '%-agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "\n",
    "save_to_db_job_name = \"api_ai4ops_push_nd_metrics_to_mysql_agg_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments = {\"--metrics_path\": f\"{DEDUPLICATION_OUT}/chunk*\",\\\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://{BUCKET}/resources/{DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--start_from\":DATA_START_DATE,\\\n",
    "            \"--metric_filter\": METRIC_FILTER\n",
    "            }\n",
    "\n",
    "save_to_db_job = builder.task_script(f'{SCRIPT_PATH}/nd_to_mysql.py')\\\n",
    ".job_id(save_to_db_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/augmentation.py')\\\n",
    ".jar(f'gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "save_to_db_executor = DataprocExecutor(save_to_db_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res = save_to_db_executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = save_to_db_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_push_nd_to_db = {\n",
    "    \"PUSH_TO_DB_JOB\": save_to_db_job_name,\n",
    "    \"PUSH_TO_DB_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_push_nd_to_db_agg.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_push_nd_to_db)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction ND Metrics to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FILTER = '%-trans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_db_job_name = \"api_ai4ops_push_nd_metrics_to_mysql_tx_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments = {\"--metrics_path\": f\"{DEDUPLICATION_OUT}/chunk*\",\\\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://{BUCKET}/resources/{DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--start_from\":DATA_START_DATE,\\\n",
    "            \"--metric_filter\": METRIC_FILTER\n",
    "            }\n",
    "\n",
    "save_to_db_job = builder.task_script(f'{SCRIPT_PATH}/nd_to_mysql.py')\\\n",
    ".job_id(save_to_db_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/augmentation.py')\\\n",
    ".jar(f'gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "save_to_db_executor = DataprocExecutor(save_to_db_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res = save_to_db_executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = save_to_db_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_push_nd_to_db = {\n",
    "    \"PUSH_TO_DB_JOB\": save_to_db_job_name,\n",
    "    \"PUSH_TO_DB_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_push_nd_to_db_tx.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_push_nd_to_db)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"signature\"></a>\n",
    "## Prepare ND LGBM Signature\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_PLATFORM_REGION = 'us-central1'\n",
    "AI_PLATFORM_MODEL_BASE_VERSION = 'v1'\n",
    "CLUSTER = 'ai4ops'\n",
    "\n",
    "OUTPUT_PATH = 'nd_models/lgbm/input'\n",
    "USE_POWER_TRANSFORMER = True\n",
    "WITH_CALENDAR_FEATURES = False\n",
    "SUFFIX = \"\"\n",
    "SCALER_NAME = \"\"\n",
    "\n",
    "# DATA_CONFIG = 'lgbm_signature_nd_peak_august_cpu.json'\n",
    "DATA_CONFIG = 'lgbm_signature_nd_peak_august_mb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = get_transition('api_transition_remove_duplicates.json')\n",
    "print(transition)\n",
    "DEDUPLICATION_OUT = transition.get('REMOVE_DUPLICATES_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "\n",
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "sign_job_name = \"api_ai4ops_nd_lgbm_signature_${}_{}\".format(SUFFIX, TIMESTAMP)\n",
    "\n",
    "SIGNATURE_OUT = f\"{OUTPUT_PATH}/{sign_job_name}\"\n",
    "\n",
    "arguments = {\"--input_data_path\":DEDUPLICATION_OUT,\\\n",
    "             \"--config\": DATA_CONFIG,\\\n",
    "             \"--output_bucket\":BUCKET,\\\n",
    "             \"--output_bucket_project\": PROJECT,\\\n",
    "            \"--output_bucket_path\":SIGNATURE_OUT,\\\n",
    "             \"--workflow_id\":str(TIMESTAMP),\\\n",
    "             \"--with_calendar_features\": WITH_CALENDAR_FEATURES, \\\n",
    "            \"--with_power_transform\": USE_POWER_TRANSFORMER \\\n",
    "            }\n",
    "\n",
    "if SCALER_NAME:\n",
    "    arguments['scaler_name'] = SCALER_NAME\n",
    "\n",
    "signature_job = builder.job_file(f'{SCRIPT_PATH}/partial_signature_lgbm.py')\\\n",
    ".job_id(sign_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{DATA_CONFIG}')\\\n",
    ".file(SCALER_NAME)\\\n",
    ".arguments(**arguments)\\\n",
    "\n",
    ".build_job()\n",
    "\n",
    "\n",
    "\n",
    "signature_executor = DataprocExecutor(signature_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_res = signature_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = signature_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_signature = {\n",
    "    \"SIGNATURE_JOB_ID\": sign_job_name,\n",
    "    \"SIGNATURE_TIMESTAMP\": TIMESTAMP,\n",
    "    \"SIGNATURE_BUCKET\": BUCKET,\n",
    "    \"SIGNATURE_TRAIN\": f\"{SIGNATURE_OUT}/LGBM-TRAIN-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_VAL\": f\"{SIGNATURE_OUT}/LGBM-VAL-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_TEST\": f\"{SIGNATURE_OUT}/LGBM-TEST-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_SCALER\": f\"{SIGNATURE_OUT}/LGBM-SCL-{TIMESTAMP}.pkl\",\n",
    "    \"SIGNATURE_DROP_KEYS\": f\"{SIGNATURE_OUT}/LGBM-DROP-KEYS-{TIMESTAMP}.txt\",\n",
    "    \"SIGNATURE_STATE\": state\"\n",
    "}\n",
    "\n",
    "print(transition_signature)\n",
    "\n",
    "with open('api_transition_nd_signature.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_signature)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hptuning\"></a>\n",
    "## Train and Tune LGBM Model\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = f\"{PROJECT_PATH}/models/gcp/lightgbm\"\n",
    "signature_transition = get_transition('api_transition_nd_signature.json')\n",
    "print(signature_transition)\n",
    "state = signature_transition.get('SIGNATURE_STATE', '')\n",
    "print('State: {}'.format(state))\n",
    "if state not in ['DONE']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "\n",
    "SIGNATURE_TRAIN = transition.get('SIGNATURE_TRAIN', '')\n",
    "SIGNATURE_VAL = transition.get('SIGNATURE_VAL', '')\n",
    "SIGNATURE_TEST = transition.get('SIGNATURE_TEST', '')\n",
    "SIGNATURE_SCALER = transition.get('SIGNATURE_SCALER', '')\n",
    "CATEGORICAL_COLUMNS = 'metric_class'\n",
    "EXCLUDED_COLUMNS =  'time,metric,metric_id'\n",
    "\n",
    "TUNING_CONFIG_FILE='hptuning_config_nd.yaml'\n",
    "TRAIN_JOB_SUFFIX = 'nd_peak_august_m\n",
    "WAIT_DELAY='60'\n",
    "WAIT_TRIES='6'\n",
    "SCALE_TIER=\"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyyaml\n",
    "\n",
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "TUNING_JOB_NAME=f\"api_ai4ops_tuning_lgbm_{TRAIN_JOB_SUFFIX}_{TIMESTAMP}\"\n",
    "JOB_DIR=f\"gs://{BUCKET}/nd_models/lgbm/models/lightgbm/{TUNING_JOB_NAME}\"\n",
    "ERR_LOG_PATH_GS=f\"nd_models/lgbm/models/lightgbm/{TUNING_JOB_NAME}/output\"\n",
    "TRAINED_MODEL_PATH_GS = f\"nd_models/lgbm/models/lightgbm/{TUNING_JOB_NAME}/model\"\n",
    "\n",
    "training_input = {\n",
    "  \"scaleTier\": SCALE_TIER,\n",
    "  \"masterType\":\"large_model\",\\\n",
    "  \"workerType\":\"large_model\",\\\n",
    "  \"parameterServerType\":\"large_model\",\\\n",
    "  \"workerCount\":\"4\",\\\n",
    "  \"parameterServerCount\":\"3\",\\\n",
    "  \"masterConfig\": {\n",
    "    \"imageUri\": \"gcr.io/kohls-kos-cicd/ai4ops_lgbm_image\"\n",
    "  },\n",
    "  \"region\": AI_PLATFORM_REGION,\n",
    "  \"jobDir\": JOB_DIR\n",
    "}\n",
    "\n",
    "args = {\n",
    "    '--is_hyperparameters_tuning': True,\\\n",
    "    '--bucket_id': BUCKET, \\\n",
    "  '--train_data_path_gs': SIGNATURE_TRAIN, \\\n",
    "  '--val_data_path_gs': SIGNATURE_VAL, \\\n",
    "  '--err_log_path_gs': ERR_LOG_PATH_GS, \\\n",
    "  '--trained_model_path_gs': TRAINED_MODEL_PATH_GS, \\\n",
    "  '--boosting_type': \"gbdt\", \\\n",
    "  '--num_leaves': 7, \\\n",
    "  '--learning_rate': 0.0215553547770489, \\\n",
    "  '--subsample_for_bin': 200000,\\\n",
    "  '--objective': \"huber\", \\\n",
    "  '--eval_metric': \"mae\", \\\n",
    "  '--obj_penalty': 1, \\\n",
    "  '--metrics': \"l1,l2\", \\\n",
    "  '--min_split_gain'\"\" 0.00021777905410443098, \\\n",
    "  '--min_child_weight': 15.29904104261325, \\\n",
    "  '--min_child_samples': 171, \\\n",
    "  '--subsample': 0.7424239139415899, \\\n",
    "  '--subsample_freq': 57, \\\n",
    "  '--colsample_bytree': 0.4449652059931909, \\\n",
    "  '--n_jobs': -1, \\\n",
    "  '--early_stopping_rounds'; 10, \\\n",
    "  '--importance_type': \"split\", \\\n",
    "  '--categorical_feature': CATEGORICAL_COLUMNS, \\\n",
    "  '--target': \"var1(t)\", \\\n",
    "  '--excluded': EXCLUDED_COLUMNS\n",
    "}\n",
    "\n",
    "\n",
    "ai_tuning_job = AIJob(TUNING_JOB_NAME, training_input)\n",
    "\n",
    "\n",
    "ai_tuning_job.set_args(args)\n",
    "ai_tuning_job.load_hyperparameters_from_file(TUNING_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_executor = AIPlatformJobExecutor(ai_tuning_job, 60, 60)\n",
    "\n",
    "response = tuning_executor.submit_train_job()\n",
    "state = response.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_tuning = {\n",
    "    \"TRAIN_JOB_ID\": TUNING_JOB_NAME,\n",
    "    \"TRAIN_JOB_DIR\": JOB_DIR,\n",
    "    \"TRAIN_STATE\": state,\n",
    "    \"TRAINED_MODEL\": f\"{JOB_DIR}/model\",\n",
    "    \"IS_TUNING\":True\n",
    "}\n",
    "\n",
    "print(transition_tuning)\n",
    "\n",
    "with open('api_transition_tuning.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_tuning)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deployment\"></a>\n",
    "## Trained Model Deployment\n",
    "See AI Platform Models https://console.cloud.google.com/mlengine/models\n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = f\"{PROJECT_PATH}/models/gcp/lightgbm\"\n",
    "train_out = get_transition('api_transition_tuning.json')\n",
    "\n",
    "TRAIN_JOB_ID = train_out.get('TRAIN_JOB_ID', '')\n",
    "TRAIN_JOB_DIR = train_out.get('TRAIN_JOB_DIR', '')\n",
    "TRAINED_MODEL = train_out.get('TRAINED_MODEL', '')\n",
    "IS_TUNING = train_out.get('IS_TUNING', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNATURE_SCALER = signature_transition.get('SIGNATURE_SCALER', '')\n",
    "SIGNATURE_DROP_KEYS = signature_transition.get('SIGNATURE_DROP_KEYS', '')\n",
    "SIGNATURE_BUCKET = signature_transition.get('SIGNATURE_BUCKET', '')\n",
    "\n",
    "DEPLOYMENT_PATH = 'deployment'\n",
    "\n",
    "MODEL_NAME = AI_PALTFORM_MODEL_NAME\n",
    "VERSION_NAME = AI_PLATFORM_MODEL_BASE_VERSION\n",
    "OBJECTIVE_VALUE_IS_MAXIMUM_NEEDED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_job_input = {\n",
    "  'pythonVersion': \"3.5\", \\\n",
    "  'deploymentUri': TRAINED_MODEL,\\\n",
    "  'packageUris': [f'{STAGING_DIR}/{PREDICTOR_PACKAGE}'], \\\n",
    "  'autoScaling':{'minNodes':1},\n",
    "  'runtimeVersion': '1.13'\n",
    "  'predictionClass': 'custom_predictor.LGBMPredictor'\n",
    "}\n",
    "custom_predictor_setup_path = f\"{SCRIPT_PATH}/setup.py\"\n",
    "\n",
    "deploy_job = AIJob(\"\", deploy_input = deploy_job_input, hp_tununing=True)\n",
    "\n",
    "artefacts_map = {\n",
    "    SIGNATURE_SCALER:'scaler.pkl',\\\n",
    "    SIGNATURE_DROP_KEYS:'drop_keys.txt'\n",
    "}\n",
    "\n",
    "artefacts_path = f'{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_executor = AIPlatformJobExecutor(session, deploy_job, 20,5)\n",
    "deploy_executor.submit_deploy_model_job(MODEL_NAME, VERSION_NAME, artefacts_path, artefacts_map, TRAIN_JOB_ID, custom_predictor_setup_path=f\"{SCRIPT_PATH}/setup.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_deployment = {\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"VERSION_NAME\": VERSION_NAME,\n",
    "    \"MODEL_DIR\": TRAINED_MODEL\",\n",
    "    \"STAGING_DIR\": STAGING_DIR,\n",
    "    \"DEPLOYMENT\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}\",\n",
    "    \"SCALER\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/scaler.pkl\",\n",
    "    \"DROP_KEYS\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/drop_keys.txt\"\n",
    "}\n",
    "\n",
    "print(transition_deployment)\n",
    "\n",
    "with open('api_transition_deployment.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_deployment)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"coldstart\"></a>\n",
    "## Cold Start Prediction before Anomaly Detection\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = f'{PROJECT_BASE_PATH}/spark/ingest'\n",
    "DURATION = '10000'\n",
    "CONFIG_NAME = 'lgbm_moving_anomalies_detection_ext_sparse_4m_20190827.json'\n",
    "POOL_SIZE = '4' \n",
    "CHUNK_SIZE = '10'\n",
    "COLD_START_FROM = '-1h'\n",
    "COLD_START_TO= '0m'\n",
    "COLD_START_STEP = '60m'\n",
    "COLD_START_STEP_DELAY = '0m'\n",
    "METRIC_DB_TABLE = 'metric_rt'\n",
    "PREDICTION_DB_TABLE = 'prediction_rt_synthetic'\n",
    "DEPLOYMENT_PATH=\"deployment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "\n",
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "COLD_START_JOB_NAME = f\"api_ai4ops_cold_start_prediction_{TIMESTAMP}\"\n",
    "\n",
    "arguments = {\n",
    "             '--gs_bucket':BUCKET,\\\n",
    "             '--gs_base_deployment_path':DEPLOYMENT_PATH,\\\n",
    "             '--tasks_file_path':CONFIG_NAME,\\\n",
    "             '--db_credentials_file_gcs_path': f\"gs://{BUCKET}/resources/{DB_SECRET}\" \\\n",
    "             '--res_path':RESOURCES,\\\n",
    "             '--duration':DURATION,\\\n",
    "             '--pool_size':POOL_SIZE,\\\n",
    "             '--project_id':PROJECT,\\\n",
    "             '--metric_db_table':METRIC_DB_TABLE,\\\n",
    "             '--prediction_db_table':PREDICTION_DB_TABLE,\\\n",
    "             '--cold_start_from':COLD_START_FROM,\\\n",
    "             '--cold_start_to':COLD_START_TO,\\\n",
    "             '--cold_start_step':COLD_START_STEP,\\\n",
    "             '--cold_start_step_delay':COLD_START_STEP_DELAY,\\\n",
    "             '--chunk_size':CHUNK_SIZE\n",
    "            }\n",
    "            \n",
    "\n",
    "cold_start_job = builder.job_file(f'{SCRIPT_PATH}/cold_start_prediction.py')\\\n",
    ".job_id(COLD_START_JOB_NAME)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/signature_lgbm.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/anomaly_detection.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_streaming_alerts.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_streaming_moving_anomalies.py')\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{CONFIG_NAME}')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".arguments(**arguments)\\\n",
    ".property('spark.executor.memory','6G'),\\\n",
    ".property('spark.num.executors','4'),\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "cold_start_executor = DataprocExecutor(cold_start_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_start_res = cold_start_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = cold_start_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prediction\"></a>\n",
    "## Batch Prediction\n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = f\"{PROJECT_PATH}/models/gcp/lightgbm/ai_platform_predictions\"\n",
    "deployment = get_transition('api_transition_deployment.json')\n",
    "AI_PALTFORM_MODEL_NAME = deployment.get('MODEL_NAME', '')\n",
    "AI_PALTFORM_MODEL_VERSION = deployment.get('VERSION_NAME', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "BASE_PATH=\"apigee_history/apigee/metrics/history\"\n",
    "OUTPUT_DATA_PATH=f\"{BASE_PATH}/lgbm-batch-predicted/{TIMESTAMP}\"\n",
    "MAX_PARALLEL_REQUESTS=4\n",
    "\n",
    "EXCLUDED_INPUT_COLUMNS=\"time,metric_val,metric,var1(t)\"\n",
    "PREDICTED_COLUMN_NAME=\"predicted\"\n",
    "OUTPUT_COLUMNS_MAPPING=\"metric_val=metric,time,var1(t)=value\"\n",
    "\n",
    "PREDICT_JOB_ID=f\"api_ai4ops_batch_prediction_lgbm_{TIMESTAMP}\"\n",
    "\n",
    "\n",
    "arguments = {\"--project_id\":PROJECT,\\\n",
    "             \"--bucket_name\": BUCKET,\\\n",
    "             \"--model_name\":AI_PALTFORM_MODEL_NAME,\\\n",
    "             \"--version_name\": AI_PALTFORM_MODEL_VERSION,\\\n",
    "            \"--input_data_file\":SIGNATURE_TEST,\\\n",
    "             \"--output_data_path\":OUTPUT_DATA_PATH,\\\n",
    "             \"--excluded_input_columns\": EXCLUDED_INPUT_COLUMNS, \\\n",
    "            \"--predicted_column_name\": PREDICTED_COLUMN_NAME \\\n",
    "            \"--output_columns_mapping\": OUTPUT_COLUMNS_MAPPING, \\\n",
    "            \"--samples_count_in_chunk\": 800, \\\n",
    "            \"--max_parallel_requests\": MAX_PARALLEL_REQUESTS,\n",
    "             \"--drop_by_nan_columns\" : \"\"\n",
    "            }\n",
    "\n",
    "predict_job = builder.job_file(f'{SCRIPT_PATH}/batch_predictions.py')\\\n",
    ".job_id(PREDICT_JOB_ID)\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "predict_executor = DataprocExecutor(predict_job, session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_res = predict_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = predict_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prediction = {\n",
    "    \"PREDICTION_JOB_ID\": PREDICT_JOB_ID,\n",
    "    \"PREDICTION_TIMESTAMP\": TIMESTAMP,\n",
    "    \"PREDICTION_BUCKET\": BUCKET,\n",
    "    \"PREDICTION_OUTPUT\": OUTPUT_DATA_PATH,\n",
    "    \"PREDICTION_STATE\": state,\n",
    "    \"PREDICTION_EXCLUDED_INPUT_COLUMNS\": EXCLUDED_INPUT_COLUMNS,\n",
    "    \"PREDICTION_PREDICTED_COLUMN_NAME\": PREDICTED_COLUMN_NAME,\n",
    "    \"PREDICTION_OUTPUT_COLUMNS_MAPPING\": OUTPUT_COLUMNS_MAPPING\n",
    "}\n",
    "\n",
    "print(transition_prediction)\n",
    "\n",
    "with open('api_transition_prediction.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_prediction)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"detection\"></a>\n",
    "## Batch Anomaly Detection for Test\n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = '/home/jovyan/work/data/poc'\n",
    "SCRIPT_PATH = f\"{PROJECT_PATH}/spark/ingest\"\n",
    "prediction = get_transition('api_transition_prediction.json')\n",
    "\n",
    "state = transition.get('PREDICTION_STATE', '')\n",
    "print('State: {}'.format(state))\n",
    "if state not in ['DONE']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "    \n",
    "PREDICTION_BUCKET= prediction.get('PREDICTION_BUCKET', '')\n",
    "PREDICTION_OUTPUT = prediction.get('PREDICTION_OUTPUT', \n",
    "                                   \n",
    "THRESHOLD = 5\n",
    "# USE_INVERSE_TRANSFORM = False\n",
    "\n",
    "ANOMALIES_BASE_VERSION = 'ND MB Peak & August' \n",
    "ANALYTICS_PATH = 'apigee_history/apigee/metrics/history/lgbm-analytics'\n",
    "                                   \n",
    "INVERSE_TRANSFORMER_PATH = (transition_deployment.get('SCALER', '') if USE_INVERSE_TRANSFORM \n",
    "                                          else '')\n",
    "STAT_DEPLOYMENT_PATH = \"deployment/{}_{}\".format(transition_deployment.get('MODEL_NAME', ''), \n",
    "                                                               transition_deployment.get('VERSION_NAME', ''))\n",
    "if not INVERSE_TRANSFORMER_PAT:\n",
    "    STAT_FILE=\"stat.csv\"\n",
    "else\n",
    "    STAT_FILE=\"inverse_stat.csv\"                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "\n",
    "DETECTION_JOB_ID=f\"api_ai4ops_anomaly_detection_{TIMESTAMP}\"\n",
    "\n",
    "VERSION=f\"Ver.{TIMESTAMP}: {ANOMALIES_BASE_VERSION} THRE{THRESHOLD}\"\n",
    "DB_SECRET=\"kohls_db.txt\"\n",
    "RESOURCES=\"/opt/dataproc/.resources\"\n",
    "PREDICTION_PATH=f\"gs://{PREDICTION_BUCKET}/{PREDICTION_OUTPUT}\"\n",
    "ANOMALY_NEIGHBORHOOD_SIZE = 1\n",
    "\n",
    "arguments = {\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://${BUCKET}/resources/${DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--predictions_path\": PREDICTION_PATH, \\\n",
    "            \"--inverse_transformer_name\": GCPHelper.get_file_name(INVERSE_TRANSFORMER_PATH),\\\n",
    "            \"--anomaly_threshold\":THRESHOLD,\\\n",
    "            \"--anomaly_neighborhood_size\": ANOMALY_NEIGHBORHOOD_SIZE, \\\n",
    "            \"--version\": VERSION \\\n",
    "            \"--output_analytics_project\": PROJECT, \\\n",
    "            \"--output_analytics_bucket\": BUCKET, \\\n",
    "            \"--output_analytics_path\": f\"{ANALYTICS_PATH}/{TIMESTAMP}\",\n",
    "            \"--output_stat_project\" : PROJECT,\\\n",
    "            \"--output_stat_bucket\" : BUCKET,\\\n",
    "            \"--output_stat_path\" : STAT_DEPLOYMENT_PATH,\\\n",
    "            \"--output_stat_file\" : STAT_FILE\n",
    "            }\n",
    "\n",
    "detect_job = builder.job_file(f'{SCRIPT_PATH}/anomaly_detection.py')\\\n",
    ".job_id(DETECTION_JOB_ID)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_plotter.py')\\\n",
    ".file(INVERSE_TRANSFORMER_PATH)\\\n",
    ".jars(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "detect_executor = DataprocExecutor(detect_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_res = detect_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = detect_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_anomalies = {\n",
    "    \"ANOMALIES_JOB_ID\": DETECTION_JOB_ID,\n",
    "    \"ANOMALIES_TIMESTAMP\": TIMESTAMP,\n",
    "    \"ANOMALIES_ANALYTICS\": BUCKET,\n",
    "    \"ANOMALIES_STATE\": state,\n",
    "    \"ANOMALIES_VERSION\": VERSION,\n",
    "    \"STAT_PATH\": f\"gs://{BUCKET}/{STAT_DEPLOYMENT_PATH}/${STAT_FILE}\"\n",
    "}\n",
    "\n",
    "print(transition_prediction)\n",
    "\n",
    "with open('api_transition_anomalies.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_anomalies)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALIES_VERSION = transition.get('ANOMALIES_VERSION', '')\n",
    "ANOMALIES_EXAMPLE_METRIC = 'cncui_blue-available_memory_mb-agg'\n",
    "\n",
    "ANOMALIES_ANALYTICS = transition.get('ANOMALIES_ANALYTICS', '')\n",
    "ANOMALIES_VERSION_ENC = ANOMALIES_VERSION.replace(' ', '%20')\n",
    "GRAFANA_REF = ('<a id=\"grafana\"></a><h2>Grafana Anomalies Dashboard</h2><a href=\"http://ai4ops-grafana-0:8080/d/1rJbPKnzWk1/ai4ops-versioned-anomalies?orgId=1&' + \n",
    "            'var-anomaly_metric_name={}&var-anomaly_version={}\">{}</a>'.format(ANOMALIES_EXAMPLE_METRIC, \n",
    "                                                                           ANOMALIES_VERSION_ENC,\n",
    "                                                                             ANOMALIES_VERSION))\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(GRAFANA_REF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analytics\"></a>\n",
    "## Analytics\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shutil.rmtree('anomalies_analytics')\n",
    "os.mkdir('anomalies_analytics')\n",
    "\n",
    "GCPHelper.download_folder_from_gs(BUCKET, ANOMALIES_ANALYTICS, 'anomalies_analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLinks('anomalies_analytics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"streaming\"></a>\n",
    "## Streaming\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"\"\n",
    "\n",
    "DB_SECRET=\"kohls_db.txt\"\n",
    "ND_SECRET=\"kohls_nd.txt\"\n",
    "BATCH_DURATION = \"\"\n",
    "\n",
    "arguments = {'--token_file_gcs_path':f'gs://{BUCKET}/resources/kohls_nd.txt',\\\n",
    "             '--db_credentials_file_gcs_path': f\"gs://{BUCKET}/resources/{DB_SECRET}\" \\\n",
    "             '--res_path':RESOURCES,\\\n",
    "             '--duration':DURATION,\\\n",
    "             '--pool_size':POOL_SIZE,\\\n",
    "             '--batch_duration':BATCH_DURATION,\\\n",
    "             '--metric_db_table':'metric_rt'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "\n",
    "\n",
    "STREAMING_JOB_ID=f\"ai4ops_streaming_nd_ingest_{TIMESTAMP}\"\n",
    "\n",
    "streaming_job = builder.job_file(f'{SCRIPT_PATH}/nd_streaming_ingest.py')\\\n",
    ".job_id(DETECTION_JOB_ID)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/nd_ingest.py')\\\n",
    ".file(CONFIG_PATH)\\\n",
    ".jars(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".max_failures(-1)\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "streaming_executor = DataprocExecutor(detect_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_res = streaming_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = streaming_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gcs\"></a>\n",
    "## Push Notebook to GCS Bucket\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_NOTEBOOKS_PATH = 'ai4ops-source/ai4ops-jupyter-ds-01'\n",
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "PROJECT = 'kohls-kos-cicd'\n",
    "upload_file_to_gs(PROJECT, BUCKET, './api_net_diagnostics_ingest.ipynb', GS_NOTEBOOKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
