{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Anomalies Detection Workflow with LightGBM Framework, Google AI Platfrom and Dataproc</center></h1>\n",
    "<a id=\"tc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. [Configuration](#configuration) \n",
    "2. [Prepare TRAIN, VAL, TEST data for LightGBM Model](#preparation)\n",
    "3. [Train LightGBM Model](#train)<br/>\n",
    "    3.1. [Train With Default Parameters](#train_with_default)<br/>\n",
    "    3.2. [Hyperparameters Tuning](#tuning)<br/>\n",
    "4. [Batch Prediction with TEST Dataset](#prediction)\n",
    "5. [Anomalies Detection with TEST Dataset. Publish to Grafana](#detection)\n",
    "6. [Grafana Anomalies Dashboard](#grafana)\n",
    "7. [Analytics](#analytics)\n",
    "8. [Push Notebook to GCS Bucket](#gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "## Configuration\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(transition_file):\n",
    "    with open(transition_file, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'kohls-kos-cicd'\n",
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "AI_PLATFORM_REGION = 'us-central1'\n",
    "DATA_BASE_PATH = \"apigee_history/apigee/metrics/history\"\n",
    "REGION='global'\n",
    "SCRIPT_PATH = 'poc/spark/ingest'\n",
    "AI4OPS_HISTORY_PATH=f\"gs://{BUCKET}/apigee_history/apigee/metrics/history\"\n",
    "PROJECT_BASE_PATH='poc'\n",
    "INPUT_DATA_SOURCE_PATH = ('{}/no_duplicates/ai4ops_remove_duplicates_1559167473,\\\n",
    "                          {}/no_duplicates/ai4ops_remove_duplicates_1559168049,\\\n",
    "                          {}/no_duplicates/ai4ops_remove_duplicates_1559168353'\n",
    "                          .format(AI4OPS_HISTORY_PATH, \n",
    "                                  AI4OPS_HISTORY_PATH, \n",
    "                                  AI4OPS_HISTORY_PATH))\n",
    "\n",
    "DATA_CONFIG = 'lgbm_signature_march_april_may_traffic_not_200.json'\n",
    "\n",
    "\n",
    "TRAIN_JOB_SUFFIX = 'traffic_not_200_march_april_may'\n",
    "TUNING_CONFIG_FILE = 'hptuning_config_march_traffic_not_200.yaml' \n",
    "AI_PALTFORM_MODEL_NAME = 'api_ai4ops_lgbm_traffic_not_200_march_april_may'\n",
    "\n",
    "AI_PLATFORM_TRAIN_EXCLUDED_COLUMNS = 'time,metric_val,metric'\n",
    "AI_PLATFORM_PREDICT_EXCLUDED_INPUT_COLUMNS = '{},var1(t)'.format(AI_PLATFORM_TRAIN_EXCLUDED_COLUMNS)\n",
    "AI_PLATFORM_PREDICTED_COLUMN_NAME = 'predicted'\n",
    "AI_PLATFORM_PREDICT_OUTPUT_COLUMNS_MAPPING = 'metric_val=metric,time,var1(t)=value'\n",
    "\n",
    "AI_PLATFORM_MODEL_BASE_VERSION = 'v1'\n",
    "CLUSTER = 'ai4ops'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparation\"></a>\n",
    "## Prepare TRAIN, VAL, TEST data for LightGBM Model \n",
    "See Dataproc Jobs: https://console.cloud.google.com/dataproc/jobs\n",
    "<br/>[back to Table Of Contents](#tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '{}/lgbm-signature'.format(DATA_BASE_PATH)\n",
    "USE_POWER_TRANSFORMER = False\n",
    "USE_INVERSE_TRANSFORM = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_api import *\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = 'traffic_not_200'\n",
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "sign_job_name = \"ai4ops_lgbm_signature_{}_{}\".format(SUFFIX, TIMESTAMP)\n",
    "\n",
    "SIGNATURE_OUT = f\"{OUTPUT_PATH}/{sign_job_name}\"\n",
    "\n",
    "arguments = {\"--input_data_path\":INPUT_DATA_SOURCE_PATH,\\\n",
    "             \"--signature_task\": DATA_CONFIG,\\\n",
    "             \"--output_bucket\":BUCKET,\\\n",
    "             \"--output_bucket_project\": PROJECT,\\\n",
    "            \"--output_bucket_path\":SIGNATURE_OUT,\\\n",
    "             \"--workflow_id\":str(TIMESTAMP),\\\n",
    "             \"--with_calendar_features\": False, \\\n",
    "            \"--with_power_transform\": False \\\n",
    "            \"--scaler_name\": None, \\\n",
    "            \"--moving_average_window_size\": None, \\\n",
    "            \"--cut_outliers_percentage\": None\n",
    "            }\n",
    "\n",
    "signature_job = builder.job_file(f'{SCRIPT_PATH}/signature_lgbm.py')\\\n",
    ".job_id(sign_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{DATA_CONFIG}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "\n",
    "signature_executor = DataprocExecutor(signature_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_res = signature_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = signature_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_signature = {\n",
    "    \"SIGNATURE_JOB_ID\": sign_job_name,\n",
    "    \"SIGNATURE_TIMESTAMP\": TIMESTAMP,\n",
    "    \"SIGNATURE_BUCKET\": BUCKET,\n",
    "    \"SIGNATURE_TRAIN\": f\"{SIGNATURE_OUT}/LGBM-TRAIN-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_VAL\": f\"{SIGNATURE_OUT}/LGBM-VAL-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_TEST\": f\"{SIGNATURE_OUT}/LGBM-TEST-{TIMESTAMP}.csv\",\n",
    "    \"SIGNATURE_SCALER\": f\"{SIGNATURE_OUT}/LGBM-SCL-{TIMESTAMP}.pkl\",\n",
    "    \"SIGNATURE_DROP_KEYS\": f\"{SIGNATURE_OUT}/LGBM-DROP-KEYS-{TIMESTAMP}.txt\",\n",
    "    \"SIGNATURE_STATE\": state\"\n",
    "}\n",
    "\n",
    "print(transition_signature)\n",
    "\n",
    "with open('api_transition_signature.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_signature)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Train LightGBM Model\n",
    "\n",
    "See https://console.cloud.google.com/mlengine/jobs\n",
    "\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"poc/models/gcp/lightgbm\"\n",
    "signature_transition = get_transition('api_transition_signature.json')\n",
    "print(signature_transition)\n",
    "state = signature_transition.get('SIGNATURE_STATE', '')\n",
    "print('State: {}'.format(state))\n",
    "if state not in ['DONE']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "\n",
    "SIGNATURE_TRAIN = transition.get('SIGNATURE_TRAIN', '')\n",
    "SIGNATURE_VAL = transition.get('SIGNATURE_VAL', '')\n",
    "SIGNATURE_TEST = transition.get('SIGNATURE_TEST', '')\n",
    "SIGNATURE_SCALER = transition.get('SIGNATURE_SCALER', '')\n",
    "CATEGORICAL_COLUMNS = ''\n",
    "EXCLUDED_COLUMNS = AI_PLATFORM_TRAIN_EXCLUDED_COLUMNS\n",
    "\n",
    "SCALE_TIER = 'STANDARD_1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_with_default\"></a>\n",
    "### Train With Default Parameters\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOW_TUNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "TRAIN_JOB_NAME=f\"api_ai4ops_lgbm_{TIMESTAMP}\"\n",
    "JOB_DIR=f\"gs://{BUCKET}/apigee_history/apigee/models/lightgbm/{TRAIN_JOB_NAME}\"\n",
    "ERR_LOG_PATH_GS=f\"apigee_history/apigee/models/lightgbm/{TRAIN_JOB_NAME}/output\"\n",
    "TRAINED_MODEL_PATH_GS = f\"apigee_history/apigee/models/lightgbm/{TRAIN_JOB_NAME}/model\"\n",
    "\n",
    "training_input = {\n",
    "  \"scaleTier\": SCALE_TIER,\n",
    "  \"masterConfig\": {\n",
    "    \"imageUri\": \"gcr.io/kohls-kos-cicd/ai4ops_lgbm_image\"\n",
    "  },\n",
    "  \"region\": AI_PLATFORM_REGION,\n",
    "  \"jobDir\": JOB_DIR\n",
    "}\n",
    "\n",
    "args = {\n",
    "    '--bucket_id': BUCKET, \\\n",
    "  '--train_data_path_gs': SIGNATURE_TRAIN, \\\n",
    "  '--val_data_path_gs': SIGNATURE_VAL, \\\n",
    "  '--train_val_data_folders_prefix_gs': \"\", \\\n",
    "  '--err_log_path_gs': ERR_LOG_PATH_GS, \\\n",
    "  '--trained_model_path_gs': TRAINED_MODEL_PATH_GS, \\\n",
    "  '--is_incremental_training': False, \\\n",
    "  '--boosting_type': \"gbdt\", \\\n",
    "  '--num_leaves': 742, \\\n",
    "  '--max_depth': 24, \\\n",
    "  '--learning_rate': 0.077748582911491323, \\\n",
    "  '--n_estimators': 153, \\\n",
    "  '--subsample_for_bin': 200000, \\\n",
    "  '--objective': \"mse\", \\\n",
    "  '--eval_metric': \"mae\", \\\n",
    "  '--obj_penalty': 1, \\\n",
    "  '--metrics': \"l1,l2\", \\\n",
    "  '--min_split_gain'\"\" 0.034919206912700895, \\\n",
    "  '--min_child_weight': 26.516502633376557, \\\n",
    "  '--min_child_samples': 1, \\\n",
    "  '--subsample': 0.89397221915183522, \\\n",
    "  '--subsample_freq': 38, \\\n",
    "  '--colsample_bytree': 0.53285653662681576, \\\n",
    "  '--reg_alpha': 34.255124175030247, \\\n",
    "  '--reg_lambda': 18.894649785140459, \\\n",
    "  '--n_jobs': -1, \\\n",
    "  '--early_stopping_rounds'; 10, \\\n",
    "  '--importance_type': \"split\", \\\n",
    "  '--categorical_feature': CATEGORICAL_COLUMNS, \\\n",
    "  '--target': \"var1(t)\", \\\n",
    "  '--excluded': EXCLUDED_COLUMNS\n",
    "}\n",
    "\n",
    "\n",
    "ai_train_job = AIJob(TRAIN_JOB_NAME, training_input)\n",
    "\n",
    "ai_train_job.set_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session = Session(BUCKET, AI_PLATFORM_REGION, CLUSTER, PROJECT)\n",
    "defaulf_train_executor = AIPlatformJobExecutor(train_session, ai_train_job)\n",
    "\n",
    "response=defaulf_train_executor.submit_train_job(30, 20)\n",
    "\n",
    "state = response.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_train = {\n",
    "    \"TRAIN_JOB_ID\": TRAIN_JOB_NAME,\n",
    "    \"TRAIN_JOB_DIR\": JOB_DIR,\n",
    "    \"TRAIN_STATE\": state,\n",
    "    \"TRAINED_MODEL\": f\"{JOB_DIR}/model\"\n",
    "}\n",
    "\n",
    "print(transition_train)\n",
    "\n",
    "with open('api_transition_train.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "### Hyperparameters Tuning\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOW_TUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyyaml\n",
    "\n",
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "TUNING_JOB_NAME=f\"api_ai4ops_tuning_lgbm_{TIMESTAMP}\"\n",
    "JOB_DIR=f\"gs://{BUCKET}/apigee_history/apigee/models/lightgbm/{TUNING_JOB_NAME}\"\n",
    "ERR_LOG_PATH_GS=f\"apigee_history/apigee/models/lightgbm/{TUNING_JOB_NAME}/output\"\n",
    "TRAINED_MODEL_PATH_GS = f\"apigee_history/apigee/models/lightgbm/{TUNING_JOB_NAME}/model\"\n",
    "\n",
    "training_input = {\n",
    "  \"scaleTier\": SCALE_TIER,\n",
    "  \"masterConfig\": {\n",
    "    \"imageUri\": \"gcr.io/kohls-kos-cicd/ai4ops_lgbm_image\"\n",
    "  },\n",
    "  \"region\": AI_PLATFORM_REGION,\n",
    "  \"jobDir\": JOB_DIR\n",
    "}\n",
    "\n",
    "args = {\n",
    "    '--bucket_id': BUCKET, \\\n",
    "    '--is_hyperparameters_tuning': True,\\\n",
    "  '--train_data_path_gs': SIGNATURE_TRAIN, \\\n",
    "  '--val_data_path_gs': SIGNATURE_VAL, \\\n",
    "  '--train_val_data_folders_prefix_gs': \"\", \\\n",
    "  '--err_log_path_gs': ERR_LOG_PATH_GS, \\\n",
    "  '--trained_model_path_gs': TRAINED_MODEL_PATH_GS, \\\n",
    "  '--is_incremental_training': False, \\\n",
    "  '--boosting_type': \"gbdt\", \\\n",
    "  '--num_leaves': 7, \\\n",
    "  '--learning_rate': 0.077748582911491323, \\\n",
    "  '--subsample_for_bin': 200000,\\\n",
    "  '--objective': \"mse\", \\\n",
    "  '--eval_metric': \"mae\", \\\n",
    "  '--obj_penalty': 1, \\\n",
    "  '--metrics': \"l1,l2\", \\\n",
    "  '--min_split_gain'\"\" 0.00021777905410443098, \\\n",
    "  '--min_child_weight': 15.299041042613257, \\\n",
    "  '--min_child_samples': 171, \\\n",
    "  '--subsample': 0.7424239139415899, \\\n",
    "  '--subsample_freq': 57, \\\n",
    "  '--colsample_bytree': 0.4449652059931909, \\\n",
    "  '--n_jobs': -1, \\\n",
    "  '--early_stopping_rounds'; 10, \\\n",
    "  '--importance_type': \"split\", \\\n",
    "  '--categorical_feature': CATEGORICAL_COLUMNS, \\\n",
    "  '--target': \"var1(t)\", \\\n",
    "  '--excluded': EXCLUDED_COLUMNS\n",
    "}\n",
    "\n",
    "\n",
    "ai_tuning_job = AIJob(TUNING_JOB_NAME, training_input)\n",
    "\n",
    "\n",
    "ai_tuning_job.set_args(args)\n",
    "ai_tuning_job.load_hyperparameters_from_file(TUNING_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_executor = AIPlatformJobExecutor(train_session, ai_tuning_job)\n",
    "\n",
    "response = tuning_executor.submit_train_job(60, 60)\n",
    "state = response.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_tuning = {\n",
    "    \"TRAIN_JOB_ID\": TUNING_JOB_NAME,\n",
    "    \"TRAIN_JOB_DIR\": JOB_DIR,\n",
    "    \"TRAIN_STATE\": state,\n",
    "    \"TRAINED_MODEL\": f\"{JOB_DIR}/model\"\n",
    "}\n",
    "\n",
    "print(transition_tuning)\n",
    "\n",
    "with open('api_transition_tuning.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_tuning)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deployment\"></a>\n",
    "## Trained Model Deployment\n",
    "See AI Platform Models https://console.cloud.google.com/mlengine/models\n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"poc/models/gcp/lightgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALLOW_TUNING:\n",
    "    train_out = get_transition('api_transition_tuning.json')\n",
    "else:\n",
    "    train_out = get_transition('api_transition_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_out.get('TRAIN_STATE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JOB_ID = train_out.get('TRAIN_JOB_ID', '')\n",
    "TRAIN_JOB_DIR = train_out.get('TRAIN_JOB_DIR', '')\n",
    "TRAINED_MODEL = train_out.get('TRAINED_MODEL', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALLOW_TUNING:\n",
    "    OBJECTIVE_VALUE_IS_MAXIMUM_NEEDED = False\n",
    "    PRINT_INFO = True\n",
    "    best_trial = AIPlatformJobExecutor\n",
    "    .get_best_hp_tuning_result(train_session._cloudml, PROJECT, TRAIN_JOB_ID, OBJECTIVE_VALUE_IS_MAXIMUM_NEEDED, PRINT_INFO)\n",
    "     \n",
    "    \n",
    "    job_api.delete_path_from_gs(BUCKET, TRAINED_MODEL)\n",
    "    \n",
    "    TRAINED_MODEL=f\"{TRAIN_JOB_DIR}/model_trial_{best_trial}\"\n",
    "    \n",
    "    job_api.copy_folder_gs(BUCKET, TRAINED_MODEL, f\"{TRAIN_JOB_DIR}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"poc/models/gcp/lightgbm/ai_platform_predictions\"\n",
    "SIGNATURE_SCALER = signature_transition.get('SIGNATURE_SCALER', '')\n",
    "SIGNATURE_DROP_KEYS = signature_transition.get('SIGNATURE_DROP_KEYS', '')\n",
    "SIGNATURE_BUCKET = signature_transition.get('SIGNATURE_BUCKET', '')\n",
    "\n",
    "SCALER_GCS_PATH = f'gs://{SIGNATURE_BUCKET}/{SIGNATURE_SCALER}'\n",
    "DROP_KEYS_GCS_PATH = f'gs://{SIGNATURE_BUCKET}/{SIGNATURE_DROP_KEYS}'\n",
    "DEPLOYMENT_PATH = f'gs://{SIGNATURE_BUCKET}/deployment'\n",
    "\n",
    "MODEL_NAME = AI_PALTFORM_MODEL_NAME\n",
    "VERSION_NAME = AI_PLATFORM_MODEL_BASE_VERSION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "PREDICTOR_PACKAGE_NAME=\"custom_predictor\"\n",
    "PREDICTOR_PACKAGE_VERSION=\"0.1\"\n",
    "PREDICTOR_PACKAGE=\"${PREDICTOR_PACKAGE_NAME}-${PREDICTOR_PACKAGE_VERSION}.tar.gz\"\n",
    "STAGING_DIR = f'{TRAINED_MODEL}/staging'\n",
    "DIST_DIR=\"dist\"\n",
    "\n",
    "script_args = ['sdist', '--formats=gztar', f'--dist-dir={DIST_DIR}'] \n",
    "\n",
    "job_api.create_package(f\"{SCRIPT_PATH}/setup.py\", )\n",
    "\n",
    "\n",
    "job_api.upload_file_to_gs(PROJECT, BUCKET, f'{DIST_DIR}/{PREDICTOR_PACKAGE}', STAGING_DIR)\n",
    "\n",
    "\n",
    "shutil.rmtree(DIST_DIR)\n",
    "shutil.rmtree(f'{PREDICTOR_PACKAGE_NAME}.egg-info')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deplpoy_job_input = {\n",
    "  'pythonVersion': \"3.5\", \\\n",
    "  'deploymentUri': TRAINED_MODEL, \\\n",
    "  'packageUris': [f'{STAGING_DIR}/{PREDICTOR_PACKAGE}'], \\\n",
    "  'autoScaling':{'minNodes':1},\n",
    "  'runtimeVersion': '1.13'\n",
    "  'predictionClass': 'custom_predictor.LGBMPredictor'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIPlatformJobExecutor\n",
    ".submit_deploy_model_job(train_session, MODEL_NAME, VERSION_NAME, deplpoy_job_input, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_api.copy_file_gs(BUCKET,SCALER_GCS_PATH, f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/scaler.pkl\")\n",
    "job_api.copy_file_gs(BUCKET,DROP_KEYS_GCS_PATH, f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/drop_keys.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_deployment = {\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"VERSION_NAME\": VERSION_NAME,\n",
    "    \"MODEL_DIR\": TRAINED_MODEL\",\n",
    "    \"STAGING_DIR\": STAGING_DIR,\n",
    "    \"DEPLOYMENT\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}\",\n",
    "    \"SCALER\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/scaler.pkl\",\n",
    "    \"DROP_KEYS\": f\"{DEPLOYMENT_PATH}/{MODEL_NAME}_{VERSION_NAME}/drop_keys.txt\"\n",
    "}\n",
    "\n",
    "print(transition_deployment)\n",
    "\n",
    "with open('api_transition_deployment.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_deployment)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prediction\"></a>\n",
    "## Batch Prediction with TEST Dataset\n",
    "See Dataproc Jobs: https://console.cloud.google.com/dataproc/jobs \n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"{}/models/gcp/lightgbm/ai_platform_predictions\".format(PROJECT_BASE_PATH)\n",
    "deployment = get_transition('api_transition_deployment.json')\n",
    "AI_PALTFORM_MODEL_NAME = deployment.get('MODEL_NAME', '')\n",
    "AI_PALTFORM_MODEL_VERSION = deployment.get('VERSION_NAME', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "BASE_PATH=\"apigee_history/apigee/metrics/history\"\n",
    "OUTPUT_DATA_PATH=f\"{BASE_PATH}/lgbm-batch-predicted/{TIMESTAMP}\"\n",
    "MAX_PARALLEL_REQUESTS=4\n",
    "\n",
    "EXCLUDED_INPUT_COLUMNS=\"time,metric_val,var1(t)\"\n",
    "PREDICTED_COLUMN_NAME=\"predicted\"\n",
    "OUTPUT_COLUMNS_MAPPING=\"metric_val=metric,time,var1(t)=value\"\n",
    "\n",
    "PREDICT_JOB_ID=f\"api_ai4ops_batch_prediction_lgbm_{TIMESTAMP}\"\n",
    "\n",
    "\n",
    "arguments = {\"--project_id\":PROJECT,\\\n",
    "             \"--bucket_name\": BUCKET,\\\n",
    "             \"--model_name\":AI_PALTFORM_MODEL_NAME,\\\n",
    "             \"--version_name\": AI_PALTFORM_MODEL_VERSION,\\\n",
    "            \"--input_data_file\":SIGNATURE_TEST,\\\n",
    "             \"--output_data_path\":OUTPUT_DATA_PATH,\\\n",
    "             \"--excluded_input_columns\": EXCLUDED_INPUT_COLUMNS, \\\n",
    "            \"--predicted_column_name\": PREDICTED_COLUMN_NAME \\\n",
    "            \"--output_columns_mapping\": OUTPUT_COLUMNS_MAPPING, \\\n",
    "            \"--samples_count_in_chunk\": 800, \\\n",
    "            \"--max_parallel_requests\": MAX_PARALLEL_REQUESTS,\n",
    "             \"--drop_by_nan_columns\" : \"\"\n",
    "            }\n",
    "\n",
    "predict_job = builder.job_file(f'{SCRIPT_PATH}/batch_predictions.py')\\\n",
    ".job_id(PREDICT_JOB_ID)\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "predict_executor = DataprocExecutor(predict_job, session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_res = predict_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = predict_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prediction = {\n",
    "    \"PREDICTION_JOB_ID\": PREDICT_JOB_ID,\n",
    "    \"PREDICTION_TIMESTAMP\": TIMESTAMP,\n",
    "    \"PREDICTION_BUCKET\": BUCKET,\n",
    "    \"PREDICTION_OUTPUT\": OUTPUT_DATA_PATH,\n",
    "    \"PREDICTION_STATE\": state,\n",
    "    \"PREDICTION_EXCLUDED_INPUT_COLUMNS\": EXCLUDED_INPUT_COLUMNS,\n",
    "    \"PREDICTION_PREDICTED_COLUMN_NAME\": PREDICTED_COLUMN_NAME,\n",
    "    \"PREDICTION_OUTPUT_COLUMNS_MAPPING\": OUTPUT_COLUMNS_MAPPING\n",
    "}\n",
    "\n",
    "print(transition_prediction)\n",
    "\n",
    "with open('api_transition_prediction.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_prediction)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"detection\"></a>\n",
    "## Anomalies Detection with TEST Dataset. Publish to Grafana\n",
    "See Dataproc Jobs: https://console.cloud.google.com/dataproc/jobs \n",
    "<br/>[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_BASE_PATH=\"poc\"\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "prediction = get_transition('api_transition_prediction.json')\n",
    "\n",
    "state = transition.get('PREDICTION_STATE', '')\n",
    "print('State: {}'.format(state))\n",
    "if state not in ['DONE']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "    \n",
    "PREDICTION_BUCKET= prediction.get('PREDICTION_BUCKET', '')\n",
    "PREDICTION_OUTPUT = prediction.get('PREDICTION_OUTPUT', \n",
    "                                   \n",
    "THRESHOLD = 4.5\n",
    "# USE_INVERSE_TRANSFORM = False\n",
    "\n",
    "ANOMALIES_BASE_VERSION = 'LGBM Traffic Not 200 May Good' \n",
    "ANALYTICS_PATH = 'apigee_history/apigee/metrics/history/lgbm-analytics'\n",
    "                                   \n",
    "INVERSE_TRANSFORMER_PATH = (transition_deployment.get('SCALER', '') if USE_INVERSE_TRANSFORM \n",
    "                                          else '')\n",
    "STAT_DEPLOYMENT_PATH = \"deployment/{}_{}\".format(transition_deployment.get('MODEL_NAME', ''), \n",
    "                                                               transition_deployment.get('VERSION_NAME', ''))\n",
    "if not INVERSE_TRANSFORMER_PAT:\n",
    "    STAT_FILE=\"stat.csv\"\n",
    "else\n",
    "    STAT_FILE=\"inverse_stat.csv\"                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP=int(datetime.now().timestamp())\n",
    "\n",
    "DETECTION_JOB_ID=f\"api_ai4ops_anomaly_detection_{TIMESTAMP}\"\n",
    "\n",
    "VERSION=f\"Ver.{TIMESTAMP}: {ANOMALIES_BASE_VERSION} THRE{THRESHOLD}\"\n",
    "DB_SECRET=\"kohls_db.txt\"\n",
    "RESOURCES=\"/opt/dataproc/.resources\"\n",
    "PREDICTION_PATH=f\"gs://{PREDICTION_BUCKET}/{PREDICTION_OUTPUT}\"\n",
    "ANOMALY_NEIGHBORHOOD_SIZE = 1\n",
    "\n",
    "arguments = {\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://${BUCKET}/resources/${DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--predictions_path\": PREDICTION_PATH, \\\n",
    "            \"--inverse_transformer_name\": job_api.get_file_name(INVERSE_TRANSFORMER_PATH),\\\n",
    "            \"--anomaly_threshold\":THRESHOLD,\\\n",
    "            \"--anomaly_neighborhood_size\": ANOMALY_NEIGHBORHOOD_SIZE, \\\n",
    "            \"--version\": VERSION \\\n",
    "            \"--output_analytics_project\": PROJECT, \\\n",
    "            \"--output_analytics_bucket\": BUCKET, \\\n",
    "            \"--output_analytics_path\": f\"{ANALYTICS_PATH}/{TIMESTAMP}\",\n",
    "            \"--output_stat_project\" : PROJECT,\\\n",
    "            \"--output_stat_bucket\" : BUCKET,\\\n",
    "            \"--output_stat_path\" : STAT_DEPLOYMENT_PATH,\\\n",
    "            \"--output_stat_file\" : STAT_FILE\n",
    "            }\n",
    "\n",
    "detect_job = builder.job_file(f'{SCRIPT_PATH}/anomaly_detection.py')\\\n",
    ".job_id(DETECTION_JOB_ID)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_plotter.py')\\\n",
    ".file(INVERSE_TRANSFORMER_PATH)\\\n",
    ".jars(f\"gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar\")\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "detect_executor = DataprocExecutor(detect_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_res = detect_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "state = detect_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_anomalies = {\n",
    "    \"ANOMALIES_JOB_ID\": DETECTION_JOB_ID,\n",
    "    \"ANOMALIES_TIMESTAMP\": TIMESTAMP,\n",
    "    \"ANOMALIES_ANALYTICS\": BUCKET,\n",
    "    \"ANOMALIES_STATE\": state,\n",
    "    \"ANOMALIES_VERSION\": VERSION,\n",
    "    \"STAT_PATH\": f\"gs://{BUCKET}/{STAT_DEPLOYMENT_PATH}/${STAT_FILE}\"\n",
    "}\n",
    "\n",
    "print(transition_prediction)\n",
    "\n",
    "with open('api_transition_anomalies.json', 'w') as file:\n",
    "     file.write(json.dumps(transition_anomalies)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = get_transition('api_transition_anomalies.json')\n",
    "print(transition)\n",
    "state = transition.get('ANOMALIES_STATE', '')\n",
    "print('State: {}'.format(state))\n",
    "if state not in ['DONE']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "ANOMALIES_VERSION = transition.get('ANOMALIES_VERSION', '')\n",
    "\n",
    "ANOMALIES_ANALYTICS = transition.get('ANOMALIES_ANALYTICS', '')\n",
    "ANOMALIES_VERSION = ANOMALIES_VERSION\n",
    "ANOMALIES_VERSION_ENC = ANOMALIES_VERSION.replace(' ', '%20')\n",
    "GRAFANA_REF = ('<a id=\"grafana\"></a><h2>Grafana Anomalies Dashboard</h2><a href=\"http://ai4ops-grafana-0:8080/d/1rJbPKnzWk1/ai4ops-versioned-anomalies?orgId=1&' + \n",
    "            'var-anomaly_metric_name={}&var-anomaly_version={}\">{}</a>'.format(ANOMALIES_EXAMPLE_METRIC, \n",
    "                                                                           ANOMALIES_VERSION_ENC,\n",
    "                                                                             ANOMALIES_VERSION))\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(GRAFANA_REF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analytics\"></a>\n",
    "## Analytics\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shutil.rmtree('anomalies_analytics')\n",
    "os.mkdir('anomalies_analytics')\n",
    "\n",
    "job_api.download_folder_from_gs(BUCKET, ANOMALIES_ANALYTICS, 'anomalies_analytics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLinks('anomalies_analytics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gcs\"></a>\n",
    "## Push Notebook to GCS Bucket\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp api_anomalies_batch_workflow.ipynb gs://ai4ops-main-storage-bucket/ai4ops-source/ai4ops-jupyter-ds-03/api/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
