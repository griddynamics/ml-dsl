{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Ingest History Data from Apigee with Dataproc Cluster</center></h1>\n",
    "<a id=\"tc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. [Configuration](#configuration) \n",
    "2. [History Ingest to Google Cloud Storage](#ingest)\n",
    "3. [Save states of jobs to GCS](#states)\n",
    "4. [Push Notebook to GCS Bucket](#gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "## Configuration\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/jdk1.8.0_221'\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"JAVA_HOME\"] + '/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "PROJECT = 'kohls-kos-cicd'\n",
    "CLUSTER = 'ai4ops'\n",
    "REGION='global'\n",
    "SCRIPT_PATH = 'poc/spark/ingest'\n",
    "AI4OPS_HISTORY_PATH=f\"gs://{BUCKET}/apigee_history/apigee/metrics/history\"\n",
    "RESOURCES='/opt/dataproc/.resources'\n",
    "\n",
    "INGEST_JOB_1='job_part_kohls_06_01.json'\n",
    "INGEST_JOB_2='job_part_kohls_06_02.json'\n",
    "INGEST_JOB_3='job_part_kohls_06_03.json'\n",
    "\n",
    "arguments = {'--token_file_gcs_path':f'gs://{BUCKET}/resources/a_with_proxy_v2.0.txt',\\\n",
    "             '--res_path':RESOURCES,\\\n",
    "             '--by_proxy':'50',\n",
    "             '--by_time':'480',\n",
    "             '--batch_size':'50'\\\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ingest\"></a>\n",
    "## History Ingest to Google Cloud Storage\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_api import *\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyspark\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7f52b4774f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script --name yarn_logging.py --path poc/spark/ingest\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "class YarnLogger:\n",
    "    @staticmethod\n",
    "    def setup_logger():\n",
    "        if not 'LOG_DIRS' in os.environ:\n",
    "            sys.stderr.write('Missing LOG_DIRS environment variable, pyspark logging disabled')\n",
    "            return\n",
    "\n",
    "        file = os.environ['LOG_DIRS'].split(',')[0] + '/pyspark.log'\n",
    "        logging.basicConfig(filename=file, level=logging.INFO,\n",
    "                            format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s')\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return getattr(logging, key)\n",
    "\n",
    "\n",
    "YarnLogger.setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_1_name = \"api_ai4ops_history_ingest_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments['--tasks_file_path'] = f'./{INGEST_JOB_1}'\n",
    "arguments['--output_file_pattern_path'] = f'{AI4OPS_HISTORY_PATH}/{job_1_name}'\n",
    "\n",
    "job1 = builder.job_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".job_id(job_1_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_script('yarn_logging.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{INGEST_JOB_1}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "executor1 = DataprocExecutor(job1, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_2_name = \"api_ai4ops_history_ingest_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments['--tasks_file_path'] = f'./{INGEST_JOB_2}'\n",
    "arguments['--output_file_pattern_path'] = f'{AI4OPS_HISTORY_PATH}/{job_2_name}'\n",
    "# builder = DataprocJobBuilder()\n",
    "job2 = builder.job_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".job_id(job_2_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_script('yarn_logging.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{INGEST_JOB_2}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "executor2 = DataprocExecutor(job2, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_3_name = \"api_ai4ops_history_ingest_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments['--tasks_file_path'] = f'./{INGEST_JOB_3}'\n",
    "arguments['--output_file_pattern_path'] = f'{AI4OPS_HISTORY_PATH}/{job_3_name}'\n",
    "# builder = DataprocJobBuilder()\n",
    "job3 = builder.job_file(f'{SCRIPT_PATH}/apigee_history_ingest.py')\\\n",
    ".job_id(job_3_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_script('yarn_logging.py')\\\n",
    ".jar(f\"gs://{BUCKET}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    ".file(f'{SCRIPT_PATH}/jobs/{INGEST_JOB_3}')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "\n",
    "executor3 = DataprocExecutor(job3, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_ai4ops_history_ingest_1567426379 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "res1 = executor1.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_ai4ops_history_ingest_1567426380 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "res2 = executor2.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_ai4ops_history_ingest_1567426381 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "res3 = executor3.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1: RUNNING\n",
      "State 2: RUNNING\n",
      "State 3: RUNNING\n"
     ]
    }
   ],
   "source": [
    "sleep(60)\n",
    "state1 = executor1.get_job_state()\n",
    "state2 = executor2.get_job_state()\n",
    "state3 = executor3.get_job_state()\n",
    "\n",
    "print('State 1: {}'.format(state1))\n",
    "print('State 2: {}'.format(state2))\n",
    "print('State 3: {}'.format(state3))\n",
    "\n",
    "if state1 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "if state2 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n",
    "\n",
    "if state3 not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_transition = {\n",
    "    \"INGEST_JOB_1\": f\"{job1.job_id}\",\n",
    "    \"INGEST_JOB_2\": f\"{job2.job_id}\",\n",
    "    \"INGEST_JOB_3\": f\"{job3.job_id}\",\n",
    "    \"INGEST_TIMESTAMP\": f\"{int(datetime.now().timestamp())}\",\n",
    "    \"INGEST_BUCKET\": f\"{BUCKET}\",\n",
    "    \"INGEST_OUTPUT_JOB_1\": f\"{AI4OPS_HISTORY_PATH}/{job_1_name}/chunk*\",\n",
    "    \"INGEST_OUTPUT_JOB_2\": f\"{AI4OPS_HISTORY_PATH}/{job_2_name}/chunk*\",\n",
    "    \"INGEST_OUTPUT_JOB_3\": f\"{AI4OPS_HISTORY_PATH}/{job_3_name}/chunk*\",\n",
    "    \"INGEST_STATE_JOB_1\": f\"{state1}\",\n",
    "    \"INGEST_STATE_JOB_2\": f\"{state2}\",\n",
    "    \"INGEST_STATE_JOB_3\": f\"{state3}\"\n",
    "}\n",
    "\n",
    "with open('api_transition_ingest.json', 'w') as file:\n",
    "     file.write(json.dumps(ingest_transition)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RUNNING'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor1.get_job_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gcs\"></a>\n",
    "## Push Notebook to GCS Bucket\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp api_history_ingest.ipynb gs://ai4ops-main-storage-bucket/ai4ops-source/ai4ops-jupyter-ds-03/api"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
