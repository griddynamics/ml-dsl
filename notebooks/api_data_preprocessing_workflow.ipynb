{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Apigee History Data Preprocessing with Dataproc Cluster</center></h1>\n",
    "<a id=\"tc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. [Configuration](#configuration) \n",
    "2. [Source Selection](#select)\n",
    "3. [Augmentation](#augmentation)\n",
    "4. [Remove Duplicates](#rmduplicates)\n",
    "5. [Push Metrics To DB](#todb)\n",
    "6. [Push Notebook to GCS Bucket](#gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "## Configuration\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/jdk1.8.0_221'\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"JAVA_HOME\"] + '/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "PROJECT = 'kohls-kos-cicd'\n",
    "CLUSTER = 'ai4ops'\n",
    "REGION='global'\n",
    "SCRIPT_PATH = 'poc/spark/ingest'\n",
    "AI4OPS_HISTORY_PATH=f\"gs://{BUCKET}/apigee_history/apigee/metrics/history\"\n",
    "RESOURCES='/opt/dataproc/.resources'\n",
    "DATA_START_DATE = '2019-05-20T00:00:00Z'\n",
    "DATA_END_DATE = '2019-06-09T00:00:00Z'\n",
    "PUSH_TO_DB_START_FROM = '2019-05-20T00:00:00Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"select\"></a>\n",
    "## Source Selection\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_transition(transition_file):\n",
    "    with open(transition_file, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INGEST_JOB_1': 'ai4ops_history_ingest_1567422501', 'INGEST_JOB_2': 'ai4ops_history_ingest_1567422590', 'INGEST_JOB_3': 'ai4ops_history_ingest_1567422598', 'INGEST_TIMESTAMP': '1567422808', 'INGEST_BUCKET': 'ai4ops-main-storage-bucket', 'INGEST_OUTPUT_JOB_1': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/ai4ops_history_ingest_1567422501/chunk*', 'INGEST_OUTPUT_JOB_2': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/ai4ops_history_ingest_1567422590/chunk*', 'INGEST_OUTPUT_JOB_3': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/ai4ops_history_ingest_1567422598/chunk*', 'INGEST_STATE_JOB_1': 'RUNNING', 'INGEST_STATE_JOB_2': 'RUNNING', 'INGEST_STATE_JOB_3': 'RUNNING'}\n"
     ]
    }
   ],
   "source": [
    "transition = get_transition('api_transition_ingest.json')\n",
    "print(transition)\n",
    "INGEST_OUTPUT_JOB_1 = transition.get('INGEST_OUTPUT_JOB_1', '')\n",
    "INGEST_OUTPUT_JOB_2 = transition.get('INGEST_OUTPUT_JOB_2', '')\n",
    "INGEST_OUTPUT_JOB_3 = transition.get('INGEST_OUTPUT_JOB_3', '')\n",
    "INPUT_PATH = f'{INGEST_OUTPUT_JOB_1},{INGEST_OUTPUT_JOB_2},{INGEST_OUTPUT_JOB_3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_api import *\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7f0dc460fd30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script  --task --name select_sources.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from ai4ops_db import *\n",
    "from pyspark.sql.functions import col\n",
    "from job_api import Task\n",
    "\n",
    "\n",
    "class SelectTask(Task):\n",
    "    def run():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--input_data_path', type=str, help='comma separated input data paths')\n",
    "        parser.add_argument('--output_data_path', type=str, help='total base path')\n",
    "\n",
    "        args, u = parser.parse_known_args()\n",
    "\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        sc = spark.sparkContext\n",
    "        df = (spark.read.format(\"csv\").\n",
    "              option(\"header\", \"false\").\n",
    "              schema(DB.metrics_schema()).\n",
    "              option('delimiter', ',').\n",
    "              load(args.input_data_path.split(',')))\n",
    "        df.printSchema()\n",
    "        df_normal = df.filter(col('source') == 'apigee-kohls-prod')\n",
    "        df_empty = df.filter(col('source') == 'apigee-kohls-prod-empty')\n",
    "        df_error = df.filter(col('source') == 'apigee-kohls-prod-error')\n",
    "        df_normal.write.format('csv').save(args.output_data_path + '/normal')\n",
    "        df_empty.write.format('csv').save(args.output_data_path + '/empty')\n",
    "        df_error.write.format('csv').save(args.output_data_path + '/error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_job_name = \"api_ai4ops_select_source_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "SELECTION_OUT = f\"{AI4OPS_HISTORY_PATH}/selected/{sel_job_name}\"\n",
    "\n",
    "arguments = {\"--input_data_path\":INPUT_PATH,\\\n",
    "            \"--output_data_path\":SELECTION_OUT\\\n",
    "            }\n",
    "\n",
    "selection_job = builder.task_script('select_sources.py')\\\n",
    ".job_id(sel_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)\n",
    "\n",
    "select_executor = DataprocExecutor(selection_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_ai4ops_select_source_1567427501 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "selection_res = select_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : RUNNING\n"
     ]
    }
   ],
   "source": [
    "sleep(60)\n",
    "state = select_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading output file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'19/09/02 08:01:32 INFO org.spark_project.jetty.util.log: Logging initialized @3096ms\\n19/09/02 08:01:32 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\\n19/09/02 08:01:32 INFO org.spark_project.jetty.server.Server: Started @3218ms\\n19/09/02 08:01:32 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5edef7ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n19/09/02 08:01:33 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\\n19/09/02 08:01:33 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at ai4ops-m/10.208.107.14:8032\\n19/09/02 08:01:33 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at ai4ops-m/10.208.107.14:10200\\n19/09/02 08:01:35 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1565269688945_0429\\nTraceback (most recent call last):\\n  File \"/tmp/api_ai4ops_select_source_1567411276/run.py\", line 3, in <module>\\n    SelectTask.run()\\n  File \"/tmp/api_ai4ops_select_source_1567411276/select_sources.py\", line 28, in run\\n    load(args.input_data_path.split(\\',\\')))\\nAttributeError: \\'tuple\\' object has no attribute \\'input_data_path\\'\\n19/09/02 08:01:42 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5edef7ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_executor.download_output_from_gs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SELECT_SOURCE_JOB': 'api_ai4ops_select_source_1567427501', 'SELECT_SOURCE_OUTPUT': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/selected/api_ai4ops_select_source_1567427501', 'SELECT_SOURCE_STATE': 'RUNNING'}\n"
     ]
    }
   ],
   "source": [
    "select_transition = {\n",
    "    \"SELECT_SOURCE_JOB\": sel_job_name,\n",
    "    \"SELECT_SOURCE_OUTPUT\": SELECTION_OUT,\n",
    "    \"SELECT_SOURCE_STATE\": state\n",
    "}\n",
    "\n",
    "print (select_transition)\n",
    "\n",
    "with open('api_transition_select.json', 'w') as file:\n",
    "     file.write(json.dumps(select_transition)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"augmentation\"></a>\n",
    "## Augmentation\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7fd01fe74e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script  --task --name augmentation.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from ai4ops_db import *\n",
    "from pyspark.sql.functions import spark_partition_id, pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from datetime import datetime, timedelta\n",
    "from apigee_ingest_utils import ApigeeIngest, ISO_TIME_FORMAT\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "from job_api import Task\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.addPyFile('yarn_logging.py')\n",
    "import yarn_logging\n",
    "import gc\n",
    "\n",
    "logger = yarn_logging.YarnLogger()\n",
    "\n",
    "\n",
    "def prepare_augmented_pd_light(pdf, start, end, time_unit='m', chunk_id=''):\n",
    "    source = pdf.loc[:1, 'source'].values.tolist()[0]\n",
    "    names = pdf.groupby(['metric'], as_index=False).agg({})['metric'].values.tolist()\n",
    "    logger.info('Chunk ID: {}\\nmetrics: {},\\nsource: {}'.format(chunk_id, names, source))\n",
    "    time_range_size = int((end - start) / ApigeeIngest.delta(1, 1, time_unit)) + 1\n",
    "    time_range = [\n",
    "        (start + ApigeeIngest.delta(i, 1, time_unit)).strftime(ISO_TIME_FORMAT) for i in range(time_range_size)\n",
    "    ]\n",
    "    time_range = pd.DataFrame(time_range, columns=['time'])\n",
    "    time_range.loc[:, 'tmp'] = 1\n",
    "    time_range.loc[:, 'source_new'] = '{}-empty'.format(source)\n",
    "    gc.collect()\n",
    "    metrics = pdf.groupby(['metric'], as_index=False).agg({})\n",
    "    metrics.loc[:, 'tmp'] = 1\n",
    "    augmented = pd.merge(time_range, metrics, on=['tmp'], how='inner')\n",
    "    augmented.loc[:, 'value_new'] = None\n",
    "    gc.collect()\n",
    "\n",
    "    augmented = pd.merge(pdf, augmented, on=['time', 'metric'], how='right')\n",
    "    augmented['source'].fillna(augmented['source_new'], inplace=True)\n",
    "    augmented['value'].fillna(augmented['value_new'], inplace=True)\n",
    "    gc.collect()\n",
    "    return augmented.drop(['tmp', 'source_new', 'value_new'], axis=1)\n",
    "\n",
    "\n",
    "def prepare_augmented_udf(start, end, time_unit='m', chunk_id=''):\n",
    "    print(f'from {start} to {end}')\n",
    "    return pandas_udf(lambda p: prepare_augmented_pd_light(p, start, end, time_unit, chunk_id),\n",
    "                      returnType=DB.metrics_schema(),\n",
    "                      functionType=PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "\n",
    "class MyTask(Task):\n",
    "    def run():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--input_data_path', type=str, help='Input Data files path including wildcards', default='')\n",
    "        parser.add_argument('--output_data_path', type=str, help='Output data files path', default='')\n",
    "        parser.add_argument('--start_date', type=str, help='Epoch start date in ISO format %Y-%m-%dT%H:%M:%SZ', default='')\n",
    "        parser.add_argument('--end_date', type=str, help='Epoch end date (exclusive) in ISO format %Y-%m-%dT%H:%M:%SZ', default='')\n",
    "        args, d = parser.parse_known_args()\n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        df = (spark.read.format(\"csv\").\n",
    "              option(\"header\", \"false\").\n",
    "              schema(DB.metrics_schema()).\n",
    "              option('delimiter', ',').\n",
    "              load(args.input_data_path.split(',')))\n",
    "\n",
    "        start_time = datetime.strptime(args.start_date, ISO_TIME_FORMAT)\n",
    "        start_time = start_time.replace(tzinfo=timezone('UTC'))\n",
    "        end_time = datetime.strptime(args.end_date, ISO_TIME_FORMAT)\n",
    "        end_time = end_time.replace(tzinfo=timezone('UTC'))\n",
    "        chunk_id = '{}_{}'.format(start_time.strftime('%Y-%m-%d-%H-%M'), end_time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        chunk = (df.repartition(2000, \"metric\")\n",
    "                 .groupby(spark_partition_id())\n",
    "                 .apply(prepare_augmented_udf(start_time, end_time + timedelta(minutes=-1), time_unit='m', chunk_id=chunk_id)))\n",
    "        chunk.write.format('csv').save(args.output_data_path + '/chunk-{}'.format(chunk_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SELECT_SOURCE_JOB': 'api_ai4ops_select_source_1567411439', 'SELECT_SOURCE_OUTPUT': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/selected/api_ai4ops_select_source_1567411439', 'SELECT_SOURCE_STATE': 'DONE'}\n"
     ]
    }
   ],
   "source": [
    "transition = get_transition('api_transition_select.json')\n",
    "print(transition)\n",
    "SELECTION_OUT = transition.get('SELECT_SOURCE_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_job_name = \"augmentation_normal_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "AUGMENTATION_OUT=f'{AI4OPS_HISTORY_PATH}/normal_augmented/{aug_job_name}'\n",
    "\n",
    "arguments = {\"--input_data_path\":f\"{SELECTION_OUT}/normal\",\\\n",
    "        \"--output_data_path\":AUGMENTATION_OUT, \\\n",
    "        \"--start_date\":DATA_START_DATE,\"--end_date\":DATA_END_DATE}\n",
    "\n",
    "augumentation_job = builder.task_script('augmentation.py')\\\n",
    ".job_id(aug_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "aug_executor = DataprocExecutor(augumentation_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id augmentation_normal_1567414052 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "aug_res = aug_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : RUNNING\n"
     ]
    }
   ],
   "source": [
    "sleep(60)\n",
    "state = aug_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_executor.get_job_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmetation_transition = {\n",
    "    \"AUGMENTATION_JOB\": aug_job_name,\n",
    "    \"AUGMENTATION_OUTPUT\": AUGMENTATION_OUT,\n",
    "    \"AUGMENTATION_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_augmentation.json', 'w') as file:\n",
    "     file.write(json.dumps(augmetation_transition)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rmduplicates\"></a>\n",
    "## Remove Duplicates\n",
    "[back to Table Of Contents](#tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7fd01d01c588>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script --task --name remove_duplicates.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from ai4ops_db import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, spark_partition_id\n",
    "import numpy as np\n",
    "from job_api import Task\n",
    "\n",
    "\n",
    "@pandas_udf(DB.metrics_schema(), PandasUDFType.GROUPED_MAP)\n",
    "def grouped_mean(pdf):\n",
    "    res = pdf.sort_values([DB.METRIC, DB.TIME, DB.SOURCE]).groupby([DB.METRIC, DB.TIME], as_index=False).agg({\n",
    "        DB.VALUE: np.mean,\n",
    "        DB.SOURCE: 'first'\n",
    "    })\n",
    "    return res[DB.metrics_schema_names()]\n",
    "\n",
    "class DeduplicationTask(Task):\n",
    "    def run():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--input_data_path', type=str, help='comma separated input data paths')\n",
    "        parser.add_argument('--output_data_path', type=str, help='total base path')\n",
    "\n",
    "        args, u = parser.parse_known_args()\n",
    "\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        sc = spark.sparkContext\n",
    "        df = (spark.read.format(\"csv\").\n",
    "              option(\"header\", \"false\").\n",
    "              schema(DB.metrics_schema()).\n",
    "              option('delimiter', ',').\n",
    "              load(args.input_data_path.split(',')))\n",
    "        df.printSchema()\n",
    "        # df = df.groupby('time', 'metric', 'source').agg(F.max('value').alias('value'))\n",
    "        df = df.repartition(1000, DB.METRIC).groupby(spark_partition_id()).apply(grouped_mean)\n",
    "        # df = df.groupby(DB.TIME, DB.METRIC).apply(grouped_mean)\n",
    "        df.select(DB.metrics_schema().names).write.format('csv').save(args.output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUGMENTATION_JOB': 'augmentation_normal_1567414052', 'AUGMENTATION_OUTPUT': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/normal_augmented/augmentation_normal_1567414052', 'AUGMENTATION_STATE': 'RUNNING'}\n"
     ]
    }
   ],
   "source": [
    "transition = get_transition('api_transition_augmentation.json')\n",
    "print(transition)\n",
    "AUGMENTATION_OUT = transition.get('AUGMENTATION_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_job_name = \"remove_duplicates_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "DEDUPLICATION_OUT=f'{AI4OPS_HISTORY_PATH}/no_duplicates/{dedup_job_name}'\n",
    "\n",
    "arguments = {\"--input_data_path\":f\"{AUGMENTATION_OUT}/chunk*\",\\\n",
    "        \"--output_data_path\":DEDUPLICATION_OUT}\n",
    "\n",
    "deduplication_job = builder.task_script('remove_duplicates.py')\\\n",
    ".job_id(dedup_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "dedup_executor = DataprocExecutor(deduplication_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id remove_duplicates_1567415840 was submitted to the cluster ai4ops\n"
     ]
    }
   ],
   "source": [
    "dedup_res = dedup_executor.submit_job(run_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : DONE\n"
     ]
    }
   ],
   "source": [
    "sleep(60)\n",
    "state = dedup_executor.get_job_state()\n",
    "\n",
    "print('State : {}'.format(state))\n",
    "if state not in ['DONE', 'RUNNING']:\n",
    "    raise RuntimeError('Previous workflow step was failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'augmentation.py': <job_api.PyScript at 0x7fd01fe74e80>,\n",
       " 'remove_duplicates.py': <job_api.PyScript at 0x7fd01fe74940>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplication_transition = {\n",
    "    \"REMOVE_DUPLICATES_JOB\": dedup_job_name,\n",
    "    \"REMOVE_DUPLICATES_OUTPUT\": DEDUPLICATION_OUT,\n",
    "    \"REMOVE_DUPLICATES_STATE\": state\n",
    "}\n",
    "\n",
    "with open('api_transition_remove_duplicates.json', 'w') as file:\n",
    "     file.write(json.dumps(deduplication_transition)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todb\"></a>\n",
    "## Push Metrics to DB\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May-June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SECRET=\"kohls_db.txt\"\n",
    "TPS_FILTER = '%-tps-%-proxy'\n",
    "TOTAL_LATENCY_FILTER = '%-totalLatency%'\n",
    "PUSH_TO_DB_START_FROM = '2019-05-20T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7fd01cfb7550>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script --task --name augmentation_to_mysql.py\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from ai4ops_db import DB\n",
    "from apigee_ingest_utils import ApigeeIngest\n",
    "from augmentation import augment_corrupt\n",
    "from apigee_ingest_utils import TIME_NODE\n",
    "from job_api import Task\n",
    "\n",
    "DB_TABLE = 'metric'\n",
    "DB_STATS_TABLE = 'stats'\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.addPyFile('yarn_logging.py')\n",
    "import yarn_logging\n",
    "\n",
    "logger = yarn_logging.YarnLogger()\n",
    "\n",
    "\n",
    "def store_to_db(df, table_name, db_credentials):\n",
    "    db = DB(db_credentials, db_table=table_name)\n",
    "    df.write.format(DB.DB_FORMAT).options(**db.get_spark_db_params()).mode(DB.TBL_APPEND_FORMAT).save()\n",
    "\n",
    "\n",
    "def store_stats_to_db(stats_path, db_credentials):\n",
    "    logger.info(\"Storing statistics from csv to db start\")\n",
    "    df = spark.read.csv(stats_path, header=True)\n",
    "    df = df.drop('expected_timestep_quantity')\n",
    "    df = df.toDF('metric', 'normal_all', 'normal_null', 'empty', 'error', 'timestep_quantity')\n",
    "\n",
    "    logger.info(\"Dataframe with statistics has been prepared, start storing to db\")\n",
    "    store_to_db(df, DB_STATS_TABLE, db_credentials)\n",
    "\n",
    "\n",
    "def store_metrics_to_db(metrics_path, db_credentials, start_from=None, metric_filter='%'):\n",
    "    df = spark.read.csv(metrics_path, header=False, schema=DB.metrics_schema())\n",
    "    if start_from is not None:\n",
    "        df = df.filter(\"{} >= '{}' and metric like '{}'\".format(TIME_NODE, start_from, metric_filter))\n",
    "    store_to_db(df, DB_TABLE, db_credentials)\n",
    "\n",
    "\n",
    "def augmentation_to_db(spark_session, latency_path, traffic_path, db_credentials):\n",
    "    good_latency, bad_latency, good_traffic, bad_traffic = augment_corrupt(spark_session, latency_path, traffic_path)\n",
    "\n",
    "    logger.info(\"start saving bad_latency\")\n",
    "    store_to_db(bad_latency, DB_TABLE, db_credentials)\n",
    "    logger.info(\"end saving bad_latency\")\n",
    "\n",
    "    logger.info(\"start saving bad_traffic\")\n",
    "    store_to_db(bad_traffic, DB_TABLE, db_credentials)\n",
    "    logger.info(\"end saving bad_traffic\")\n",
    "\n",
    "    logger.info(\"start saving good_latency\")\n",
    "    store_to_db(good_latency, DB_TABLE, db_credentials)\n",
    "    logger.info(\"end saving good_latency\")\n",
    "\n",
    "    logger.info(\"start saving good_traffic\")\n",
    "    store_to_db(good_traffic, DB_TABLE, db_credentials)\n",
    "    logger.info(\"end saving good_traffic\")\n",
    "\n",
    "class SaveToDbTask(Task):\n",
    "    def run():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--db_credentials_file_path', type=str, help='db credentials file path on cluster file system')\n",
    "        parser.add_argument('--db_credentials_gcs_file_path', type=str, help='db credentials file path on GCS')\n",
    "        parser.add_argument('--res_path', type=str, help='resources directory path')\n",
    "        parser.add_argument('--metrics_path', type=str, help='metrics path')\n",
    "        parser.add_argument('--start_from', type=str, help='time string')\n",
    "        parser.add_argument('--metric_filter', type=str, help='time string', default='%')\n",
    "\n",
    "        args, u = parser.parse_known_args()\n",
    "\n",
    "        if args.db_credentials_file_path is None and args.db_credentials_gcs_file_path is None:\n",
    "            print('DB credentials paths are not found')\n",
    "            exit(1)\n",
    "\n",
    "        db_credentials_file_path = args.db_credentials_file_path\n",
    "        db_credentials_gcs_file_path = args.db_credentials_gcs_file_path\n",
    "        res_path = args.res_path\n",
    "        if db_credentials_file_path is not None:\n",
    "            db_credentials = json.loads(ApigeeIngest.dcr(res_path + '/resource.txt', db_credentials_file_path).decode('utf-8'))\n",
    "        else:\n",
    "            db_credentials_rows = spark.read.text(db_credentials_gcs_file_path).collect()\n",
    "            db_credentials_file_path = 'db.txt'\n",
    "            with open(db_credentials_file_path, 'w') as f:\n",
    "                f.write(db_credentials_rows[0][0])\n",
    "            db_credentials = json.loads(ApigeeIngest.dcr(res_path + '/resource.txt', db_credentials_file_path).decode('utf-8'))\n",
    "\n",
    "        store_metrics_to_db(args.metrics_path, db_credentials, args.start_from, metric_filter=args.metric_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'REMOVE_DUPLICATES_JOB': 'remove_duplicates_1567415840', 'REMOVE_DUPLICATES_OUTPUT': 'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/no_duplicates/remove_duplicates_1567415840', 'REMOVE_DUPLICATES_STATE': 'DONE'}\n"
     ]
    }
   ],
   "source": [
    "transition = get_transition('api_transition_remove_duplicates.json')\n",
    "print(transition)\n",
    "DEDUPLICATION_OUT = transition.get('REMOVE_DUPLICATES_OUTPUT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "session = Session(BUCKET, REGION, CLUSTER, PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_db_tps_job_name = \"api_push_metrics_to_mysql_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "arguments = {\"--metrics_path\": f\"{DEDUPLICATION_OUT}/chunk*\",\\\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://{BUCKET}/resources/{DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--start_from\": PUSH_TO_DB_START_FROM, \\\n",
    "            \"--metric_filter\": TPS_FILTER\n",
    "            }\n",
    "\n",
    "save_to_db_tps_job = builder.task_script('augmentation_to_mysql.py')\\\n",
    ".job_id(save_to_db_tps_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/augmentation.py')\\\n",
    ".jar(f'gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "save_tps_executor = DataprocExecutor(save_to_db_tps_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_db_lat_job_name = \"api_push_metrics_to_mysql_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "\n",
    "arguments = {\"--metrics_path\": f\"{DEDUPLICATION_OUT}/part*\",\\\n",
    "            \"--db_credentials_gcs_file_path\" : f\"gs://{BUCKET}/resources/{DB_SECRET}\", \\\n",
    "            \"--res_path\" : RESOURCES, \\\n",
    "            \"--start_from\": PUSH_TO_DB_START_FROM, \\\n",
    "            \"--metric_filter\": TOTAL_LATENCY_FILTER\n",
    "            }\n",
    "\n",
    "save_to_db_lat_job = builder.task_script('augmentation_to_mysql.py')\\\n",
    ".job_id(save_to_db_lat_job_name)\\\n",
    ".py_file(f'{SCRIPT_PATH}/apigee_ingest_utils.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/ai4ops_db.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/yarn_logging.py')\\\n",
    ".py_file(f'{SCRIPT_PATH}/augmentation.py')\\\n",
    ".jar(f'gs://{BUCKET}/resources/mysql-connector-java-8.0.16.jar')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "save_lat_executor = DataprocExecutor(save_to_db_lat_job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_push_metrics_to_mysql_1567420284 was submitted to the cluster ai4ops\n",
      "Job STATUS was set to PENDING at 2019-09-02 10:31:30\n",
      "Job STATUS was set to SETUP_DONE at 2019-09-02 10:31:30\n",
      "      Yarn APP augmentation_to_mysql.py with STATUS ACCEPTED has PROGRESS 0\n",
      "      Yarn APP augmentation_to_mysql.py with STATUS RUNNING has PROGRESS 10\n",
      "Job STATUS was set to RUNNING at 2019-09-02 10:31:30\n",
      "      Yarn APP augmentation_to_mysql.py with STATUS FINISHED has PROGRESS 100\n",
      "Job STATUS was set to DONE at 2019-09-02 10:31:57\n"
     ]
    }
   ],
   "source": [
    "save_tps_res = save_tps_executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': {'project_id': 'kohls-kos-cicd',\n",
       "  'job_id': 'push_metrics_to_mysql_1567416573'},\n",
       " 'placement': {'cluster_name': 'ai4ops'},\n",
       " 'pyspark_job': {'main_python_file_uri': 'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/run.py',\n",
       "  'args': ['--metrics_path',\n",
       "   'gs://ai4ops-main-storage-bucket/apigee_history/apigee/metrics/history/no_duplicates/remove_duplicates_1567415840/chunk*',\n",
       "   '--db_credentials_gcs_file_path',\n",
       "   'gs://ai4ops-main-storage-bucket/resources/kohls_db.txt',\n",
       "   '--res_path',\n",
       "   '/opt/dataproc/.resources',\n",
       "   '--start_from',\n",
       "   '2019-05-20T00:00:00Z',\n",
       "   '--metric_filter',\n",
       "   '%-tps-%-proxy'],\n",
       "  'python_file_uris': ['gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/apigee_ingest_utils.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/ai4ops_db.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/yarn_logging.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/augmentation.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/augmentation_to_mysql.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/run.py',\n",
       "   'gs://ai4ops-main-storage-bucket/jobs-root/push_metrics_to_mysql_1567416573/job_api.py'],\n",
       "  'file_uris': [],\n",
       "  'jar_file_uris': ['gs://ai4ops-main-storage-bucket/resources/mysql-connector-java-8.0.16.jar'],\n",
       "  'archive_uris': [],\n",
       "  'logging_config': None,\n",
       "  'properties': {}}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_tps_executor.job_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id api_push_metrics_to_mysql_1567420285 was submitted to the cluster ai4ops\n",
      "Job STATUS was set to PENDING at 2019-09-02 10:32:23\n",
      "Job STATUS was set to SETUP_DONE at 2019-09-02 10:32:23\n",
      "      Yarn APP augmentation_to_mysql.py with STATUS RUNNING has PROGRESS 10\n",
      "Job STATUS was set to RUNNING at 2019-09-02 10:32:24\n",
      "      Yarn APP augmentation_to_mysql.py with STATUS FINISHED has PROGRESS 100\n",
      "Job STATUS was set to DONE at 2019-09-02 10:32:48\n"
     ]
    }
   ],
   "source": [
    "save_lat_res = save_lat_executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gcs\"></a>\n",
    "## Push Notebook to GCS Bucket\n",
    "[back to Table Of Contents](#tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp api_data_preprocessing_workflow.ipynb gs://ai4ops-main-storage-bucket/ai4ops-source/ai4ops-jupyter-ds-03/api\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
