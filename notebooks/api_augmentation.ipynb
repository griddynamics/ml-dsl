{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/jdk1.8.0_221'\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"JAVA_HOME\"] + '/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_api import *\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing LOG_DIRS environment variable, pyspark logging disabled"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: from 2019-05-28 08:00:00+00:00 to 2019-05-31 15:59:00+00:00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<job_api.PyScript at 0x7fe1a60bd320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script  --task --name augmentation.py --input_data_path augmentation_in --output_data_path augmentation_out --start_date 2019-05-28T08:00:00Z --end_date 2019-05-31T16:00:00Z\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from ai4ops_db import *\n",
    "from pyspark.sql.functions import spark_partition_id, pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from datetime import datetime, timedelta\n",
    "from apigee_ingest_utils import ApigeeIngest, ISO_TIME_FORMAT\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "from job_api import Task\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.addPyFile('yarn_logging.py')\n",
    "import yarn_logging\n",
    "import gc\n",
    "\n",
    "logger = yarn_logging.YarnLogger()\n",
    "\n",
    "\n",
    "def prepare_augmented_pd_light(pdf, start, end, time_unit='m', chunk_id=''):\n",
    "    source = pdf.loc[:1, 'source'].values.tolist()[0]\n",
    "    names = pdf.groupby(['metric'], as_index=False).agg({})['metric'].values.tolist()\n",
    "    logger.info('Chunk ID: {}\\nmetrics: {},\\nsource: {}'.format(chunk_id, names, source))\n",
    "    time_range_size = int((end - start) / ApigeeIngest.delta(1, 1, time_unit)) + 1\n",
    "    time_range = [\n",
    "        (start + ApigeeIngest.delta(i, 1, time_unit)).strftime(ISO_TIME_FORMAT) for i in range(time_range_size)\n",
    "    ]\n",
    "    time_range = pd.DataFrame(time_range, columns=['time'])\n",
    "    time_range.loc[:, 'tmp'] = 1\n",
    "    time_range.loc[:, 'source_new'] = '{}-empty'.format(source)\n",
    "    gc.collect()\n",
    "    metrics = pdf.groupby(['metric'], as_index=False).agg({})\n",
    "    metrics.loc[:, 'tmp'] = 1\n",
    "    augmented = pd.merge(time_range, metrics, on=['tmp'], how='inner')\n",
    "    augmented.loc[:, 'value_new'] = None\n",
    "    gc.collect()\n",
    "\n",
    "    augmented = pd.merge(pdf, augmented, on=['time', 'metric'], how='right')\n",
    "    augmented['source'].fillna(augmented['source_new'], inplace=True)\n",
    "    augmented['value'].fillna(augmented['value_new'], inplace=True)\n",
    "    gc.collect()\n",
    "    return augmented.drop(['tmp', 'source_new', 'value_new'], axis=1)\n",
    "\n",
    "\n",
    "def prepare_augmented_udf(start, end, time_unit='m', chunk_id=''):\n",
    "    print(f'from {start} to {end}')\n",
    "    return pandas_udf(lambda p: prepare_augmented_pd_light(p, start, end, time_unit, chunk_id),\n",
    "                      returnType=DB.metrics_schema(),\n",
    "                      functionType=PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "\n",
    "class MyTask(Task):\n",
    "    def run():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--input_data_path', type=str, help='Input Data files path including wildcards', default='')\n",
    "        parser.add_argument('--output_data_path', type=str, help='Output data files path', default='')\n",
    "        parser.add_argument('--start_date', type=str, help='Epoch start date in ISO format %Y-%m-%dT%H:%M:%SZ', default='')\n",
    "        parser.add_argument('--end_date', type=str, help='Epoch end date (exclusive) in ISO format %Y-%m-%dT%H:%M:%SZ', default='')\n",
    "        args, d = parser.parse_known_args()\n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        df = (spark.read.format(\"csv\").\n",
    "              option(\"header\", \"false\").\n",
    "              schema(DB.metrics_schema()).\n",
    "              option('delimiter', ',').\n",
    "              load(args.input_data_path.split(',')))\n",
    "\n",
    "        start_time = datetime.strptime(args.start_date, ISO_TIME_FORMAT)\n",
    "        start_time = start_time.replace(tzinfo=timezone('UTC'))\n",
    "        end_time = datetime.strptime(args.end_date, ISO_TIME_FORMAT)\n",
    "        end_time = end_time.replace(tzinfo=timezone('UTC'))\n",
    "        chunk_id = '{}_{}'.format(start_time.strftime('%Y-%m-%d-%H-%M'), end_time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        chunk = (df.repartition(2000, \"metric\")\n",
    "                 .groupby(spark_partition_id())\n",
    "                 .apply(prepare_augmented_udf(start_time, end_time + timedelta(minutes=-1), time_unit='m', chunk_id=chunk_id)))\n",
    "        chunk.write.format('csv').save(args.output_data_path + '/chunk-{}'.format(chunk_id))\n",
    "MyTask.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataprocJobBuilder()\n",
    "bucket = 'ai4ops-main-storage-bucket'\n",
    "project = 'kohls-kos-cicd'\n",
    "cluster = 'ai4ops'\n",
    "region='global'\n",
    "\n",
    "\n",
    "arguments = {\"--input_data_path\":f\"gs://{bucket}/jobs-root/augmentation_in\",\\\n",
    "        \"--output_data_path\":f\"gs://{bucket}/jobs-root/augmentation_out\", \\\n",
    "        \"--start_date\":\"2019-05-28T08:00:00Z\",\"--end_date\":\"2019-05-31T16:10:00Z\"}\n",
    "\n",
    "s_path = 'poc/spark/ingest/'\n",
    "\n",
    "AI4OPS_HISTORY_PATH=f\"gs://{bucket}/apigee_history/apigee/metrics/history\"\n",
    "\n",
    "job_name = \"augmentation_{}\".format(int(datetime.now().timestamp()))\n",
    "\n",
    "job = builder.task_script('augmentation.py')\\\n",
    ".job_id(job_name)\\\n",
    ".py_file(f'{s_path}apigee_ingest_utils.py')\\\n",
    ".py_file(f'{s_path}ai4ops_db.py')\\\n",
    ".py_file(f'{s_path}/yarn_logging.py')\\\n",
    ".arguments(**arguments)\\\n",
    ".build_job()\n",
    "\n",
    "session = Session(bucket, region, cluster, project)\n",
    "\n",
    "executor = DataprocExecutor(job, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job with id augmentation_1566207207 was submitted to the cluster ai4ops\n",
      "Job STATUS was set to PENDING at 2019-08-19 09:33:29\n",
      "Job STATUS was set to SETUP_DONE at 2019-08-19 09:33:29\n",
      "      Yarn APP augmentation.py with STATUS RUNNING has PROGRESS 10\n",
      "Job STATUS was set to RUNNING at 2019-08-19 09:33:30\n",
      "      Yarn APP augmentation.py with STATUS FINISHED has PROGRESS 100\n",
      "Job STATUS was set to DONE at 2019-08-19 09:41:35\n"
     ]
    }
   ],
   "source": [
    "job = executor.submit_job(run_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.download_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
