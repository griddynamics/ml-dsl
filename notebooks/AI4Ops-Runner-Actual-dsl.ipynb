{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import os\n",
    "from settings.profiles import PySparkJobProfile, Profile\n",
    "from mldsl import *\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/apolatovskaya/key.json'\n",
    "\n",
    "PROJECT_BASE_PATH = '/Users/apolatovskaya/git/ai4ops/dsl/test'\n",
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "DATA_BASE_PATH = \"nd_history\"\n",
    "CLUSTER = 'ai4ops-streaming'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming ND Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"{}/poc/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "CONFIG_NAME='job_streaming_part_kohls_nd_logged_01.json'\n",
    "DURATION = 60\n",
    "BATCH_DURATION = 60\n",
    "MAX_FAILURES_PER_HOUR = 3\n",
    "POOL_SIZE = 1\n",
    "METRIC_DB_TABLE = 'metric_rt_synthetic'\n",
    "LOG_METRIC_DB_TABLE = 'log_metric_try'\n",
    "METRIC_ALIAS_DB_TABLE = 'metric_alias'\n",
    "\n",
    "PROJECT_ID='ai4ops_streaming_nd_ingest_bg'\n",
    "DB_SECRET=\"kohls_db.txt\"\n",
    "ND_SECRET=\"kohls_nd.txt\"\n",
    "token_file_gcs_path = \"gs://{}/resources/{}\".format(BUCKET, ND_SECRET)\n",
    "db_credentials_file_gcs_path = \"gs://{}/resources/{}\".format(BUCKET, DB_SECRET)\n",
    "\n",
    "\n",
    "#job properties\n",
    "py_files = [\"apigee_ingest_utils.py\", \"ai4ops_db.py\", \n",
    "            \"yarn_logging.py\", \"apigee_history_ingest.py\",\"nd_ingest.py\"]\n",
    "PY_FILES = [os.path.join(SCRIPT_PATH, i) for i in py_files]\n",
    "FILES = [os.path.join(SCRIPT_PATH, 'jobs', CONFIG_NAME)]\n",
    "JARS = [\"gs://{0}/resources/mysql-connector-java-8.0.16.jar\".format(BUCKET), \\\n",
    "        \"gs://{0}/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar\".format(BUCKET)]\n",
    "\n",
    "properties = {\"spark.executor.cores\":\"1\",\n",
    "              \"spark.executor.memory\":\"4G\",\n",
    "              \"spark.executor.instances\":\"2\",\n",
    "              \"spark.dynamicAllocation.enabled\":\"false\",\n",
    "              \"spark.streaming.dynamicAllocation.enabled\":\"false\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_path': '/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest',\n",
       " 'bucket': 'ai4ops-main-storage-bucket',\n",
       " 'project': 'kohls-kos-cicd',\n",
       " 'cluster': 'ai4ops-streaming',\n",
       " 'region': 'global',\n",
       " 'ai_region': 'us-central1',\n",
       " 'job_prefix': 'ai4ops_streaming_nd_ingest_bg',\n",
       " 'job_async': True,\n",
       " 'py_files': ['/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/apigee_ingest_utils.py',\n",
       "  '/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/ai4ops_db.py',\n",
       "  '/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/yarn_logging.py',\n",
       "  '/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/apigee_history_ingest.py',\n",
       "  '/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/nd_ingest.py'],\n",
       " 'files': ['/Users/apolatovskaya/git/ai4ops/dsl/test/poc/spark/ingest/jobs/job_streaming_part_kohls_nd_logged_01.json'],\n",
       " 'jars': ['gs://ai4ops-main-storage-bucket/resources/mysql-connector-java-8.0.16.jar',\n",
       "  'gs://ai4ops-main-storage-bucket/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar'],\n",
       " 'properties': {'spark.executor.cores': '1',\n",
       "  'spark.executor.memory': '4G',\n",
       "  'spark.executor.instances': '2',\n",
       "  'spark.dynamicAllocation.enabled': 'false',\n",
       "  'spark.streaming.dynamicAllocation.enabled': 'false'},\n",
       " 'args': {'--tasks_file_path': 'job_streaming_part_kohls_nd_logged_01.json',\n",
       "  '--token_file_gcs_path': 'gs://ai4ops-main-storage-bucket/resources/kohls_nd.txt',\n",
       "  '--db_credentials_file_gcs_path': 'gs://ai4ops-main-storage-bucket/resources/kohls_db.txt',\n",
       "  '--res_path': '/opt/dataproc/.resources',\n",
       "  '--duration': '60',\n",
       "  '--pool_size': '1',\n",
       "  '--batch_duration': '60',\n",
       "  '--metric_db_table': 'metric_rt_synthetic',\n",
       "  '--log_metric_db_table': 'log_metric_try',\n",
       "  '--alias_table': 'metric_alias'},\n",
       " 'archives': [],\n",
       " 'logging': None,\n",
       " 'max_failures': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Profile info\n",
    "pysparkjob_profile = PySparkJobProfile(root_path=SCRIPT_PATH, bucket=BUCKET,\\\n",
    "                                       project='kohls-kos-cicd', cluster=CLUSTER,\\\n",
    "                                       region='global', ai_region='us-central1',\\\n",
    "                                       job_prefix=PROJECT_ID, job_async=True)\n",
    "pysparkjob_profile.py_files = PY_FILES\n",
    "pysparkjob_profile.files=FILES\n",
    "pysparkjob_profile.jars=JARS\n",
    "pysparkjob_profile.properties=properties\n",
    "pysparkjob_profile.max_failures = 3\n",
    "pysparkjob_profile.args = {'--tasks_file_path':CONFIG_NAME, \n",
    "                           '--token_file_gcs_path':token_file_gcs_path,\n",
    "                           '--db_credentials_file_gcs_path':db_credentials_file_gcs_path, \n",
    "                           '--res_path':'/opt/dataproc/.resources',\n",
    "                           '--duration': str(DURATION), \n",
    "                           '--pool_size': str(POOL_SIZE), \n",
    "                           '--batch_duration':str(BATCH_DURATION), \n",
    "                           '--metric_db_table': METRIC_DB_TABLE,\n",
    "                           '--log_metric_db_table': LOG_METRIC_DB_TABLE,\n",
    "                           '--alias_table': METRIC_ALIAS_DB_TABLE}\n",
    "Profile.set(PROJECT_ID, pysparkjob_profile)\n",
    "platform = Platform.GCP\n",
    "Profile.get(PROJECT_ID).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py_script.PyScript at 0x10ef49e48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%py_script --name nd_streaming_ingest_blue_green.py --path ./test/poc/spark/ingest\n",
    "# %py_load ./test/poc/spark/ingest/nd_streaming_ingest_blue_green.py\n",
    "#!/usr/bin/python\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# noinspection PyProtectedMember\n",
    "from pyspark import SparkContext, SQLContext\n",
    "# noinspection PyProtectedMember\n",
    "from pyspark.serializers import NoOpSerializer\n",
    "from pyspark.streaming import DStream, StreamingContext\n",
    "from ai4ops_db import DB\n",
    "from apigee_ingest_utils import ApigeeIngest\n",
    "from nd_ingest import (decrypt, store_nd_metrics_with_logged_agg, TIER_ALIAS_NODE, AS_LOG_NODE)\n",
    "\n",
    "ALIAS_TABLE = 'metric_alias_table'\n",
    "\n",
    "\n",
    "def read_credentials(spr, cred_file_gcs_path, prefix, resource_path):\n",
    "    cred_rows = spr.read.text(cred_file_gcs_path).collect()\n",
    "    cred_file_path = '{}.log'.format(prefix)\n",
    "    with open(cred_file_path, 'w') as fp:\n",
    "        fp.write(cred_rows[0][0])\n",
    "    return json.loads(ApigeeIngest.dcr(resource_path + '/resource.txt', cred_file_path).decode('utf-8'))\n",
    "\n",
    "\n",
    "def group_by_metric(a):\n",
    "    return a.groupby([DB.METRIC])\n",
    "\n",
    "\n",
    "def store_nd_metrics_rdd(rdd, db, metric_db_table, log_metric_db_table, tier_alias, as_log):\n",
    "    collected_rdd = rdd.collect()\n",
    "    if len(collected_rdd) == 0:\n",
    "        return\n",
    "\n",
    "    metrics, logged_metrics = store_nd_metrics_with_logged_agg(collected_rdd, tier_alias, as_log)\n",
    "\n",
    "    if metrics is not None:\n",
    "        if metrics.shape[0] > 0:\n",
    "            print('Store metrics to db. Printing 5 head metrics ...')\n",
    "            print(metrics.head(5))\n",
    "            db.direct_upsert_to_db(metrics[DB.metrics_schema().names],\n",
    "                                   group_by_metric,\n",
    "                                   metric_db_table,\n",
    "                                   DB.metrics_schema().names)\n",
    "\n",
    "    if logged_metrics is not None:\n",
    "        if logged_metrics.shape[0] > 0:\n",
    "            print('Store logged metrics to db. Printing 5 head metrics ...')\n",
    "            print(logged_metrics.head(5))\n",
    "            db.direct_upsert_to_db(logged_metrics[DB.error_metrics_schema().names],\n",
    "                                   group_by_metric,\n",
    "                                   log_metric_db_table,\n",
    "                                   DB.error_metrics_schema().names)\n",
    "\n",
    "\n",
    "class StreamUtils(object):\n",
    "    @staticmethod\n",
    "    def createStream(streaming_context, input_tasks, db_config, input_token, pool_size, duration):\n",
    "        # noinspection PyProtectedMember\n",
    "        j_duration = streaming_context._jduration(duration)\n",
    "        try:\n",
    "            # noinspection PyProtectedMember\n",
    "            helper = streaming_context._jvm.com.kohls.spark.streaming.http.nd.bluegreen.HttpNDHelper()\n",
    "        except TypeError as e:\n",
    "            if str(e) == \"'JavaPackage' object is not callable\":\n",
    "                StreamUtils._printErrorMsg(streaming_context.sparkContext)\n",
    "            raise\n",
    "\n",
    "        # noinspection PyProtectedMember\n",
    "        j_stream = helper.createStreamFromJson(streaming_context._jssc, json.dumps(input_tasks),\n",
    "                                               db_config, input_token, pool_size, j_duration)\n",
    "        stream = DStream(j_stream, streaming_context, NoOpSerializer())\n",
    "        x = stream.map(lambda v: v)\n",
    "        logger.info(type(x))\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _printErrorMsg(context):\n",
    "        print(\"Spark Streaming's DB libraries not found in class path. {}\".format(context.version))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sc = SparkContext(appName=\"nd_streaming_ingest\").getOrCreate()\n",
    "    sc.addPyFile('yarn_logging.py')\n",
    "    import yarn_logging\n",
    "\n",
    "    logger = yarn_logging.YarnLogger()\n",
    "    sql_context = SQLContext(sc)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--tasks_file_path', type=str, help='tasks file path on cluster file system')\n",
    "    parser.add_argument('--token_file_path', type=str, help='token file path')\n",
    "    parser.add_argument('--token_file_gcs_path', type=str, help='token file GCS path')\n",
    "    parser.add_argument('--db_credentials_file_gcs_path', type=str, help='db credentials file path on GCS')\n",
    "    parser.add_argument('--res_path', type=str, help='resources directory path')\n",
    "    parser.add_argument('--duration', default=60, type=int, help='check point interval in seconds')\n",
    "    parser.add_argument('--pool_size', type=int, help='Streaming pool size', default=1)\n",
    "    parser.add_argument('--batch_duration', type=int, help='Streaming batchDuration in seconds', default=1)\n",
    "    parser.add_argument('--metric_db_table', default='metric_rt_synthetic', type=str, help='DB metric table name')\n",
    "    parser.add_argument('--alias_table', default='metric_alias', type=str, help='Alias table')\n",
    "    parser.add_argument('--log_metric_db_table', default='log_metric_try', type=str, help='DB metric table name')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    token_file_path = None\n",
    "    token_file_gcs_path = None\n",
    "    tasks_file_path = None\n",
    "    output_file_pattern_path = None\n",
    "    res_path = None\n",
    "\n",
    "    ssc = StreamingContext(sc, int(args.batch_duration))\n",
    "\n",
    "    if args.tasks_file_path is None:\n",
    "        exit(1)\n",
    "\n",
    "    tasks_file_path = args.tasks_file_path\n",
    "\n",
    "    if args.token_file_path is None and args.token_file_gcs_path is None:\n",
    "        print('Token files are not found')\n",
    "        exit(1)\n",
    "\n",
    "    token_file_path = args.token_file_path\n",
    "    token_file_gcs_path = args.token_file_gcs_path\n",
    "\n",
    "    if args.res_path is None:\n",
    "        exit(1)\n",
    "\n",
    "    res_path = args.res_path\n",
    "\n",
    "    if token_file_path is not None:\n",
    "        token = json.loads(decrypt(res_path + '/resource.txt', token_file_path))\n",
    "    else:\n",
    "        token_rows = sql_context.read.text(token_file_gcs_path).collect()\n",
    "        token_file_path = 'token.log'\n",
    "        with open(token_file_path, 'w') as f:\n",
    "            f.write(token_rows[0][0])\n",
    "        token = json.loads(decrypt(res_path + '/resource.txt', token_file_path))\n",
    "\n",
    "    with open(tasks_file_path) as f:\n",
    "        tasks = json.load(f)\n",
    "    tier_alias_dict = tasks.get(TIER_ALIAS_NODE, {})\n",
    "    as_log_list = tasks.get(AS_LOG_NODE, [])\n",
    "    db_credentials_rows = sql_context.read.text(args.db_credentials_file_gcs_path).collect()\n",
    "    db_credentials_file_path = 'db.log'\n",
    "    with open(db_credentials_file_path, 'w') as f:\n",
    "        f.write(db_credentials_rows[0][0])\n",
    "    db_credentials = json.loads(\n",
    "        decrypt(args.res_path + '/resource.txt', db_credentials_file_path).decode('utf-8'))\n",
    "    db_obj = DB(db_credentials)\n",
    "    db_credentials = read_credentials(sql_context, args.db_credentials_file_gcs_path, 'db', args.res_path)\n",
    "    db_credentials[ALIAS_TABLE] = args.alias_table\n",
    "\n",
    "    stm = StreamUtils.createStream(ssc,\n",
    "                                   tasks,\n",
    "                                   db_credentials,\n",
    "                                   token,\n",
    "                                   args.pool_size,\n",
    "                                   args.duration)\n",
    "    stm.foreachRDD(lambda rdd: store_nd_metrics_rdd(rdd,\n",
    "                                                    db_obj,\n",
    "                                                    args.metric_db_table,\n",
    "                                                    args.log_metric_db_table,\n",
    "                                                    tier_alias_dict,\n",
    "                                                    as_log_list))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://console.cloud.google.com/dataproc/jobs/ai4ops_streaming_nd_ingest_bg_1574804870?project=kohls-kos-cicd&region=global\">ai4ops_streaming_nd_ingest_bg_1574804870</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/apigee_ingest_utils.py\n",
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/ai4ops_db.py\n",
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/yarn_logging.py\n",
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/apigee_history_ingest.py\n",
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/nd_ingest.py\n",
      "Uploading file to dir: jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/job_streaming_part_kohls_nd_logged_01.json\n",
      "Job with id ai4ops_streaming_nd_ingest_bg_1574804870 was submitted to the cluster ai4ops-streaming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://console.cloud.google.com/storage/browser/ai4ops/metric_rt_synthetic\\/ai4ops_streaming_nd_ingest_bg_1574804870?project=kohls-kos-cicd\">Output Data ai4ops_streaming_nd_ingest_bg_1574804870</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "cluster_name": "ai4ops-streaming",
       "driver_control_files_uri": "",
       "driver_output_resource_uri": "",
       "job_id": "ai4ops_streaming_nd_ingest_bg_1574804870",
       "project_id": "kohls-kos-cicd",
       "pyspark_job": {
        "archives": [],
        "args": [
         "--output_path",
         "gs://ai4ops/metric_rt_synthetic\\/ai4ops_streaming_nd_ingest_bg_1574804870",
         "--tasks_file_path",
         "job_streaming_part_kohls_nd_logged_01.json",
         "--token_file_gcs_path",
         "gs://ai4ops-main-storage-bucket/resources/kohls_nd.txt",
         "--db_credentials_file_gcs_path",
         "gs://ai4ops-main-storage-bucket/resources/kohls_db.txt",
         "--res_path",
         "/opt/dataproc/.resources",
         "--duration",
         "60",
         "--pool_size",
         "1",
         "--batch_duration",
         "60",
         "--metric_db_table",
         "metric_rt_synthetic",
         "--log_metric_db_table",
         "log_metric_try",
         "--alias_table",
         "metric_alias"
        ],
        "files": [
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/job_streaming_part_kohls_nd_logged_01.json"
        ],
        "jars": [
         "gs://ai4ops-main-storage-bucket/resources/mysql-connector-java-8.0.16.jar",
         "gs://ai4ops-main-storage-bucket/resources/spark.http.apigee-1.0-SNAPSHOT-jar-with-dependencies.jar"
        ],
        "main_python_file_uri": "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/nd_streaming_ingest_blue_green.py",
        "properties": [
         [
          "spark.executor.cores",
          "1"
         ],
         [
          "spark.executor.instances",
          "2"
         ],
         [
          "spark.dynamicAllocation.enabled",
          "false"
         ],
         [
          "spark.streaming.dynamicAllocation.enabled",
          "false"
         ],
         [
          "spark.executor.memory",
          "4G"
         ]
        ],
        "py_files": [
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/apigee_ingest_utils.py",
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/ai4ops_db.py",
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/yarn_logging.py",
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/apigee_history_ingest.py",
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/nd_ingest.py",
         "gs://ai4ops-main-storage-bucket/jobs-root/ai4ops_streaming_nd_ingest_bg_1574804870/nd_streaming_ingest_blue_green.py"
        ]
       },
       "start_time": "2019-11-27T00:47:59.000000",
       "status": "PENDING"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%py_data -n nd_streaming_ingest_blue_green.py -p $PROJECT_ID -pm $platform -o gs://ai4ops/metric_rt_synthetic\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use job_ai4ops_streaming_nd_ingest_bg_1574804870 instance to browse job properties.\n",
    "job_ai4ops_streaming_nd_ingest_bg_1574804870 = job_tracker['ai4ops_streaming_nd_ingest_bg_1574804870']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_nd_ingest_bluegreen.sh \"${SCRIPT_PATH}\" \"${CONFIG_NAME}\" \\\n",
    "\"${BUCKET}\" \"${CLUSTER}\" ${DURATION} ${POOL_SIZE} ${BATCH_DURATION} ${MAX_FAILURES_PER_HOUR} \\\n",
    "\"${METRIC_DB_TABLE}\" \"${LOG_METRIC_DB_TABLE}\" \"${METRIC_ALIAS_DB_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Errors HTM Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_BASE_PATH = '{}/poc'.format('/Users/arodin/Documents/projects/kohls')\n",
    "BUCKET = 'ai4ops-main-storage-bucket'\n",
    "CLUSTER = 'ai4ops-streaming'\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['CLUSTER'] = CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['BATCH_DURATION'] = '30'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['ENVIRONMENT'] = 'production'\n",
    "os.environ['PARTITIONS'] = '3'\n",
    "os.environ['PERSISTENCE'] = 'db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_log_metrics_anomaly.sh \"${SCRIPT_PATH}\" \"${BUCKET}\" \"${CLUSTER}\" \"${ENVIRONMENT}\" \\\n",
    "${BATCH_DURATION} ${DURATION} ${PARTITIONS} \"${PERSISTENCE}\" ${MAX_FAILURES_PER_HOUR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Logs Ingest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "os.environ['CLUSTER'] = 'ai4ops-streaming'\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['SUBSCRIPTION'] = 'projects/kohls-kos-cicd/subscriptions/ai4ops_logs'\n",
    "os.environ['BATCH_DURATION'] = '60'\n",
    "os.environ['BATCH_SIZE'] = '1000'\n",
    "os.environ['PARTITIONS'] = '8'\n",
    "os.environ['STREAMS'] = '2'\n",
    "os.environ['DB_SKIP_OLDER_THAN'] = '30'\n",
    "os.environ['DB_CHUNK_SIZE'] = '10000'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_logs_ingest.sh \"${SCRIPT_PATH}\" \"${BUCKET}\" \"${CLUSTER}\" \\\n",
    "\"${SUBSCRIPTION}\" ${BATCH_DURATION} ${BATCH_SIZE} ${PARTITIONS} ${STREAMS} \"${OUTPUT_MODE}\" \"${TARGET_DS}\" \\\n",
    "${DB_SKIP_OLDER_THAN} ${DB_CHUNK_SIZE} ${MAX_FAILURES_PER_HOUR}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start ND Streaming Ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "CONFIG_NAME='job_streaming_part_kohls_nd_01.json'\n",
    "os.environ['CLUSTER_S'] = 'ai4ops'\n",
    "\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['CONFIG_NAME'] = CONFIG_NAME\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['BATCH_DURATION'] = '60'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['POOL_SIZE'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_nd_ingest_bluegreen.sh \"${SCRIPT_PATH}\" \"${CONFIG_NAME}\" \\\n",
    "\"${BUCKET}\" \"${CLUSTER_S}\" ${DURATION} ${POOL_SIZE} ${BATCH_DURATION} ${MAX_FAILURES_PER_HOUR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Apigee Streaming Ingest (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLUSTER_S'] = 'ai4ops'\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "CONFIG_NAME='job_streaming_part_kohls_3m_01_ext.json'\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['CONFIG_NAME'] = CONFIG_NAME\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['CHECKPOINT_GCS_PATH'] = ''\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['POOL_SIZE'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_ingest.sh \"${SCRIPT_PATH}\" \"${CONFIG_NAME}\" \\\n",
    "\"${BUCKET}\" \"${CLUSTER_S}\" \"${DURATION}\" \"${CHECKPOINT_GCS_PATH}\" \\\n",
    "${MAX_FAILURES_PER_HOUR} ${POOL_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Apigee Streaming Ingest (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLUSTER_S'] = 'ai4ops'\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "CONFIG_NAME='job_streaming_part_kohls_3m_02_ext.json'\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['CONFIG_NAME'] = CONFIG_NAME\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['CHECKPOINT_GCS_PATH'] = ''\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['POOL_SIZE'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_ingest.sh \"${SCRIPT_PATH}\" \"${CONFIG_NAME}\" \\\n",
    "\"${BUCKET}\" \"${CLUSTER_S}\" \"${DURATION}\" \"${CHECKPOINT_GCS_PATH}\" \\\n",
    "${MAX_FAILURES_PER_HOUR} ${POOL_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLUSTER'] = 'ai4ops-streaming'\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "CONFIG_NAME='job_streaming_anomaly_analytics_nd_7d.json'\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['ANALYTICS_CONFIGURATION_NAME'] = CONFIG_NAME\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['POOL_SIZE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_anomaly_analytics.sh \"${SCRIPT_PATH}\" \"${BUCKET}\" \"${CLUSTER}\" \\\n",
    "\"${ANALYTICS_CONFIGURATION_NAME}\" ${DURATION} ${MAX_FAILURES_PER_HOUR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection (Apigee + ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLUSTER'] = 'ai4ops-streaming'\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "# CONFIG_NAME = 'partial_lgbm_moving_anomalies_detection_union_4m_with_nobg_02.json'\n",
    "# Added Likelihood for ND_CPU_NOBG\n",
    "# CONFIG_NAME = 'partial_lgbm_moving_anomalies_detection_union_4m_with_nobg_02_lkh.json'\n",
    "# Added Likelihood for ND_CPU_NOBG and ND_THROUGHPUT_NOBG\n",
    "CONFIG_NAME = 'partial_lgbm_moving_anomalies_detection_union_4m_with_nobg_03_lkh.json'\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['CONFIG'] = CONFIG_NAME\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['POOL_SIZE'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_partial_streaming_moving_anomaly_detection.sh \"${SCRIPT_PATH}\" \"${CONFIG}\" \\\n",
    "\"${BUCKET}\" \"${CLUSTER}\" ${DURATION} ${POOL_SIZE} ${MAX_FAILURES_PER_HOUR}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLUSTER'] = 'ai4ops-streaming'\n",
    "SCRIPT_PATH = \"{}/spark/ingest\".format(PROJECT_BASE_PATH)\n",
    "os.environ['SCRIPT_PATH'] = SCRIPT_PATH\n",
    "os.environ['DURATION'] = '60'\n",
    "os.environ['POOL_SIZE'] = '3'\n",
    "os.environ['EMAIL_TEMPLATE_NAME'] = 'alert_email_template_multiple.html'\n",
    "os.environ['MAX_FAILURES_PER_HOUR'] = '3'\n",
    "os.environ['ALERTS_TABLE'] = 'alerts_temp'\n",
    "os.environ['ENVIRONMENT'] = 'production'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ${SCRIPT_PATH}/submit_streaming_alerts_dsl_conf.sh \"${SCRIPT_PATH}\" \"${BUCKET}\" \"${CLUSTER}\" \\\n",
    "${DURATION} ${POOL_SIZE} \"${EMAIL_TEMPLATE_NAME}\" ${MAX_FAILURES_PER_HOUR} \"${ALERTS_TABLE}\" \"${ENVIRONMENT}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
