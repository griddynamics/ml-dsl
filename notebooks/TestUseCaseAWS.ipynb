{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 Grid Dynamics International, Inc. All Rights Reserved\n",
    "# http://www.griddynamics.com\n",
    "# Classification level: PUBLIC\n",
    "# Licensed under the Apache License, Version 2.0(the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# Id:          ML-DSL\n",
    "# Project:     ML DSL\n",
    "# Description: DSL to configure and execute ML/DS pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> ML DSL in Action </center></h1>\n",
    "\n",
    "Amazon AWS: \n",
    "1. Amazon EMR\n",
    "2. Amazon S3\n",
    "3. Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import com.griddynamics.dsl.ml.mldsl as mldsl\n",
    "from com.griddynamics.dsl.ml.mldsl import job_tracker\n",
    "from com.griddynamics.dsl.ml.settings.profiles import SageMakerProfile, PySparkJobProfile\n",
    "from com.griddynamics.dsl.ml.settings.description import Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sequences from Original Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer to Create Sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Tokenizer on ML Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%py_script --name text_tokenizer.py --path demo/scripts\n",
    "# %py_load demo/scripts/text_tokenizer.py\n",
    "#!/usr/bin/python\n",
    "from pyspark import SQLContext, SparkContext\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import argparse\n",
    "\n",
    "\n",
    "def read_glove_vecs(glove_file, output_path):\n",
    "    rdd = sc.textFile(glove_file)\n",
    "    row = Row(\"glovevec\")\n",
    "    df = rdd.map(row).toDF()\n",
    "    split_col = F.split(F.col('glovevec'), \" \")\n",
    "    df = df.withColumn('word', split_col.getItem(0))\n",
    "    df = df.withColumn('splitted', split_col)\n",
    "    vec_udf = F.udf(lambda row: [float(i) for i in row[1:]], ArrayType(FloatType()))\n",
    "    df = df.withColumn('vec', vec_udf(F.col('splitted')))\n",
    "    df = df.drop('splitted', \"glovevec\")\n",
    "    w = Window.orderBy([\"word\"])\n",
    "    qdf = df.withColumn('vec', F.concat_ws(',', 'vec')).withColumn(\"id\", F.row_number().over(w))\n",
    "    \n",
    "    path = '{}/words'.format(output_path)\n",
    "    qdf.coalesce(1).write.format('csv').option(\"sep\",\"\\t\").option('header', 'true').save(path)\n",
    "    print('Words saved to: \"{}\"'.format(path))\n",
    "    list_words = list(map(lambda row: row.asDict(), qdf.collect()))\n",
    "    word_to_vec_map = {item['word']: item['vec'] for item in list_words}\n",
    "    words_to_index = {item['word']:item[\"id\"] for item in list_words}\n",
    "    index_to_words = {item[\"id\"]: item['word'] for item in list_words}\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "def prepare_df(path, const, words_to_index):\n",
    "    rdd = sc.textFile(path)\n",
    "    row = Row(\"review\")\n",
    "    df = rdd.map(row).toDF()\n",
    "    # Clean text\n",
    "    df_clean = df.select(F.lower(F.regexp_replace(F.col('review'), \"n't\", \" n't\")).alias('review'))\n",
    "    df_clean = df_clean.select(F.lower(F.regexp_replace(F.col('review'), \"[^0-9a-zA-Z\\\\s]\", \"\")).alias('review'))\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol='review', outputCol='words_token')\n",
    "    df_words_token = tokenizer.transform(df_clean).select('words_token')\n",
    "    df_cutted = df_words_token.withColumn('length', F.size(F.col('words_token')))\n",
    "    # Replace word with it's index\n",
    "    word_udf = F.udf(lambda row: [words_to_index[w] if w in words_to_index.keys()\n",
    "                                  else words_to_index[\"unk\"] for w in row],\n",
    "                 ArrayType(IntegerType()))\n",
    "    df_stemmed = df_cutted.withColumn('words_stemmed', word_udf(F.col('words_token')))\n",
    "    return df_stemmed.withColumn(\"class\", F.lit(const))\n",
    "\n",
    "\n",
    "def save_dataset(df_pos, df_neg, path):\n",
    "    df = df_pos.union(df_neg)\n",
    "    w = Window.orderBy([\"words_stemmed\"])\n",
    "    df = df.withColumn(\"review_id\", F.row_number().over(w)).withColumn('int_seq',\n",
    "                                                                       F.concat_ws(',', 'words_stemmed'))\n",
    "    qdf = df.select(['review_id', 'int_seq', 'class'])\n",
    "    qdf.coalesce(1).write.format('csv').option('header', 'true').save(path)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_path', type=str, help=\"train positive reviews path\")\n",
    "    parser.add_argument('--test_path', type=str, help=\"test positive reviews path\")\n",
    "    parser.add_argument('--word_embeds', type=str, help=\"Path to glove word embeddings\")\n",
    "    parser.add_argument('--output_path', type=str, help=\"Sequences output path\")\n",
    "    reviews_filter = '*.txt'\n",
    "\n",
    "    args, d = parser.parse_known_args()\n",
    "    output_path = args.output_path\n",
    "    SparkContext.setSystemProperty('spark.sql.broadcastTimeout', '36000')\n",
    "    sc = SparkContext(appName=\"word_tokenizer\").getOrCreate()\n",
    "    sql = SQLContext(sc)\n",
    "\n",
    "    words_to_index, index_to_words, word_to_vec_map = read_glove_vecs(args.word_embeds, output_path)\n",
    "    \n",
    "    df_pos = prepare_df(f\"{args.train_path}/pos/{reviews_filter}\", 1, words_to_index)\n",
    "    df_neg = prepare_df(f\"{args.train_path}/neg/{reviews_filter}\", 0, words_to_index)\n",
    "    train_path = '{}/train'.format(output_path)\n",
    "    save_dataset(df_pos, df_neg, train_path)\n",
    "    print('Train saved to: \"{}\"'.format(train_path))\n",
    "\n",
    "    df_pos = prepare_df(f\"{args.test_path}/pos/{reviews_filter}\", 1, words_to_index)\n",
    "    df_neg = prepare_df(f\"{args.test_path}/neg/{reviews_filter}\", 0, words_to_index)\n",
    "    test_path = '{}/test'.format(output_path)\n",
    "    save_dataset(df_pos, df_neg, test_path)\n",
    "    print('Test saved to: \"{}\"'.format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = Platform.AWS\n",
    "profile = PySparkJobProfile(bucket='mldsl-test', \n",
    "                            cluster='j-IUSED1D1H8F6',\n",
    "                            region='global',\n",
    "                            job_prefix='movie_review_job',\n",
    "                            root_path='demo/scripts', project=None, ai_region='us-east-1', \n",
    "                            job_async=False,\n",
    "                            use_cloud_engine_credentials='SageMakerRoleMlDsl')\n",
    "profile.args=profile.load_profile_data(\"demo/spark_job_args_aws.json\")\n",
    "profile.packages=[\"com.amazonaws:aws-java-sdk-pom:1.10.34\", \"org.apache.hadoop:hadoop-aws:2.6.0\"]\n",
    "PySparkJobProfile.set('JobProfile', profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%py_data -n text_tokenizer.py -p JobProfile -pm $platform -o s3://mldsl-test/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use job_movie_review_job_1593105239 instance to browse job properties.\n",
    "#job_movie_review_job_1593105239 = job_tracker['movie_review_job_1593105239']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model to Predict Positive or Negative Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some env variables to run script locally\n",
    "os.environ['SM_MODEL_DIR'] = 'notebooks/demo/model/'\n",
    "os.environ['SM_CHANNEL_TRAINING'] = 'notebooks/demo/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%py_script -e --name pytorch_model.py --path notebooks/demo/scripts --epochs 3\n",
    "# %py_load scripts/pytorch_model.py\n",
    "#!/usr/bin/python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from shutil import copy2\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index)\n",
    "    emb_dim = word_to_vec_map.item().get(\"apple\").shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = np.float32(word_to_vec_map.item().get(word))\n",
    "    return emb_matrix\n",
    "\n",
    "def accuracy(a, b):\n",
    "    a = torch.argmax(a, dim=1)\n",
    "    b = torch.argmax(b, dim=1)\n",
    "    return torch.sum(torch.eq(a, b)).float() / list(a.size())[0]\n",
    "\n",
    "def convert_to_one_hot(Y, C=2):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def prepare_dataset(data, N, word_to_index):\n",
    "    data['int_seq'] = data['int_seq'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "    print(\"Max sequence is set to {}\".format(N))\n",
    "    data['int_seq'] = data['int_seq'].apply(lambda x: (x + [word_to_index[\"unk\"]] * N)[:N])\n",
    "    ds_x = np.asarray(list(data[\"int_seq\"]))\n",
    "    ds_y = data[\"class\"].values\n",
    "    ds_y = convert_to_one_hot(ds_y)\n",
    "    return ds_x, ds_y\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    # predict steps is output_dim lstm_size is hidden dim of LSTM cell\n",
    "    def __init__(self, word_to_vec_map, word_to_index, lstm_size=32, input_len=200):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.lstm_size = lstm_size\n",
    "        self.input_len = input_len\n",
    "        emb_matrix = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "        self.vocab_len = len(word_to_index)\n",
    "        self.emb_dim = word_to_vec_map.item().get(\"apple\").shape[0]\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_len, self.emb_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(emb_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.lstm_size, 2, dropout=0.5, batch_first=True)\n",
    "        self.out = nn.Linear(self.lstm_size, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.embedding(data.long())\n",
    "        x, _ = self.lstm(x.view(len(data), -1, self.emb_dim))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bucket = 'mldsl-test'\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    content_object = s3.Object(bucket, 'movie/word_to_index.json')\n",
    "    file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    word_to_index = json.loads(file_content)\n",
    "    \n",
    "    obj = s3.Object(bucket, 'movie/word_to_vec_map.npy')\n",
    "    with io.BytesIO(obj.get()[\"Body\"].read()) as f:\n",
    "        word_to_vec_map = np.load(f, allow_pickle=True)\n",
    "    \n",
    "    model = LSTMModel(word_to_vec_map, word_to_index)\n",
    "    with open(os.path.join(model_dir, 'movie.pth'), 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    model = model.float()\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--batch_size', type=int, default=1024)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.03)\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    np.random.seed(1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with open(os.path.join(args.train, \"word_to_index.json\"), 'r') as f:\n",
    "        word_to_index = json.load(f)\n",
    "    word_to_vec_map = np.load(os.path.join(args.train, \"word_to_vec_map.npy\"), allow_pickle=True)\n",
    "\n",
    "    model = LSTMModel(word_to_vec_map, word_to_index, lstm_size=32, input_len=150)\n",
    "    model = model.float()\n",
    "    parameters = itertools.filterfalse(lambda p: not p.requires_grad, model.parameters())\n",
    "    # Optimizer\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(parameters, lr=args.learning_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_path = os.path.join(args.train, 'train/')\n",
    "    for file in os.listdir(train_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(train_path, file))\n",
    "\n",
    "    x_train, y_train = prepare_dataset(df, 150, word_to_index)\n",
    "    \n",
    "    tensor_x = torch.tensor(x_train).float()\n",
    "    tensor_y = torch.tensor(y_train).float()\n",
    "    \n",
    "    print(\"Positive examples are {}\".format(sum(tensor_y[:, 0])))\n",
    "    print(\"Negative examples are {}\".format(sum(tensor_y[:, 1])))\n",
    "    del x_train\n",
    "    del y_train\n",
    "\n",
    "    traindataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "    trainloader = torch.utils.data.DataLoader(traindataset, shuffle=True, batch_size=args.batch_size)\n",
    "\n",
    "    epochs = args.epochs\n",
    "    running_loss = 0\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for inputs, label in trainloader:\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, label)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(running_loss / len(trainloader))\n",
    "            model.train()\n",
    "        print(f\"\"\"Epoch {epoch+1}/{epochs}..  Train loss: {np.mean(train_losses):.3f}..  Accuracy: {accuracy(label, logps):.3f}..  Time: {(time.time() - start):.2f}\"\"\")\n",
    "        running_loss = 0\n",
    "    copy2(os.path.join(args.train, \"word_to_index.json\"), os.path.join(args.model_dir, \"word_to_index.json\"))\n",
    "    copy2(os.path.join(args.train, \"word_to_vec_map.npy\"), os.path.join(args.model_dir, \"word_to_vec_map.npy\"))\n",
    "    \n",
    "    with open(os.path.join(args.model_dir, 'movie.pth'), 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "platform = Platform.AWS\n",
    "profile = SageMakerProfile(bucket='mldsl-test', cluster='ai4ops', region='us-east-1', job_prefix='mldsl_test',\n",
    "                           container=PyTorch, root_path='notebooks/demo/scripts/', \n",
    "                           framework_version='1.4.0', instance_type='ml.m4.xlarge',\n",
    "                          use_cloud_engine_credentials='SageMakerRoleMlDsl')\n",
    "\n",
    "SageMakerProfile.set('SageMakerProfile', profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%py_train -n pytorch_model.py -p SageMakerProfile -pm $platform -o s3://mldsl-test/movie/ --training s3://mldsl-test/movie/movie_review_job_1591218321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use job_mldsl_test_1593109354 instance to browse job properties.\n",
    "#\n",
    "job_mldsl_test_1593109354 = job_tracker['mldsl_test_1593109354']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = job_mldsl_test_1593109354.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "                                             endpoint_name='mldsl-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C=2):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def prepare_dataset(data, N, word_to_index):\n",
    "    data['int_seq'] = data['int_seq'].apply(lambda x: [float(i) for i in x.split(',')])\n",
    "    print(\"Max sequence is set to {}\".format(N))\n",
    "    data['int_seq'] = data['int_seq'].apply(lambda x: (x + [word_to_index[\"unk\"]] * N)[:N])\n",
    "    ints = np.random.choice(len(data), 500)\n",
    "    ds_x = np.asarray(list(data.loc[ints, \"int_seq\"]))\n",
    "    ds_y = data.loc[ints, \"class\"].values\n",
    "    ds_y = convert_to_one_hot(ds_y)\n",
    "    return ds_x, ds_y\n",
    "\n",
    "data_key = \"movie/movie_review_job_1591218321/test\"\n",
    "bucket='mldsl-test'\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "for obj in my_bucket.objects.filter(Prefix=data_key):\n",
    "    if obj.key.endswith(\"csv\"):\n",
    "        data_location = 's3://{}/{}'.format(obj.bucket_name, obj.key)\n",
    "        df = pd.read_csv(data_location)\n",
    "x_test, y_test = prepare_dataset(df, 150, word_to_index)\n",
    "y = y_test.argmax(axis=1)\n",
    "target_names = ['negative review', 'positive review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict(x_test)\n",
    "prediction = preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(y, prediction)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "classes=['Negative', 'Positive']\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',xticklabels=classes, yticklabels=classes, cmap=plt.cm.BuGn)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment of model trained outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "profile.model_data = 's3://sagemaker-us-east-1-592068120969/pytorch-training-2020-06-04-10-19-13-527/model.tar.gz'\n",
    "profile.container = PyTorchModel\n",
    "SageMakerProfile.set('TestDeploySMProfile', profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%py_deploy -n pytorch_model.py -p TestDeploySMProfile -pm $platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use job_mldsl_test_1593153514 instance to browse job properties.\n",
    "predictor=job_tracker['mldsl_test_1593153514']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict(x_test)\n",
    "prediction = preds.argmax(axis=1)\n",
    "target_names = ['negative review', 'positive review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(y, prediction)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "classes=['Negative', 'Positive']\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',xticklabels=classes, yticklabels=classes, cmap=plt.cm.BuGn)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, prediction,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
